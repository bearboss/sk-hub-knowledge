title: Kafka命令

date: 2021-05-25 15:20:37

tags: Kafka

categories: Kafka

copyright: true

sticky: 0

---

<span id="delete">

![](/images/banner/4.jpg)

</span>

<!--more-->
# spring整合
```

###producer 
server.servlet.context-path=/producer
server.port=8001

## Spring 整合 kafka
spring.kafka.bootstrap-servers=192.168.11.51:9092
## kafka producer 发送消息失败时的一个重试的次数
spring.kafka.producer.retries=0
## 批量发送数据的配置 
spring.kafka.producer.batch-size=16384
## 设置kafka 生产者内存缓存区的大小（32M）
spring.kafka.producer.buffer-memory=33554432
## kafka消息的序列化配置
spring.kafka.producer.key-serializer=org.apache.kafka.common.serialization.StringSerializer
spring.kafka.producer.value-serializer=org.apache.kafka.common.serialization.StringSerializer

# acks=0 ： 生产者在成功写入消息之前不会等待任何来自服务器的响应。
# acks=1 ： 只要集群的首领节点收到消息，生产者就会收到一个来自服务器成功响应。
# acks=-1: 表示分区leader必须等待消息被成功写入到所有的ISR副本(同步副本)中才认为producer请求成功。这种方案提供最高的消息持久性保证，但是理论上吞吐率也是最差的。

## 	这个是kafka生产端最重要的选项
spring.kafka.producer.acks=1


@Autowired
private KafkaTemplate<String, Object> kafkaTemplate;

public void sendMessage(String topic, Object object) {
  
  ListenableFuture<SendResult<String, Object>> future = kafkaTemplate.send(topic, object);
  
  future.addCallback(new ListenableFutureCallback<SendResult<String, Object>>() {
    @Override
    public void onSuccess(SendResult<String, Object> result) {
      log.info("发送消息成功: " + result.toString());
    }

    @Override
    public void onFailure(Throwable throwable) {
      log.error("发送消息失败: " + throwable.getMessage());
    }
  });
}

###consumser 

server.servlet.context-path=/consumser
server.port=8002

spring.kafka.bootstrap-servers=192.168.11.51:9092

## consumer 消息的签收机制：手工签收
spring.kafka.consumer.enable-auto-commit=false
spring.kafka.listener.ack-mode=manual
# 该属性指定了消费者在读取一个没有偏移量的分区或者偏移量无效的情况下该作何处理：
# latest（默认值）在偏移量无效的情况下，消费者将从最新的记录开始读取数据（在消费者启动之后生成的记录）
# earliest ：在偏移量无效的情况下，消费者将从起始位置读取分区的记录
spring.kafka.consumer.auto-offset-reset=earliest
## 序列化配置
spring.kafka.consumer.key-deserializer=org.apache.kafka.common.serialization.StringDeserializer
spring.kafka.consumer.value-deserializer=org.apache.kafka.common.serialization.StringDeserializer
spring.kafka.listener.concurrency=5

@Slf4j
@Component
public class KafkaConsumerService {

	@KafkaListener(groupId = "group02", topics = "topic02")
	public void onMessage(ConsumerRecord<String, Object> record, Acknowledgment acknowledgment, Consumer<?, ?> consumer) {
		log.info("消费端接收消息: {}", record.value());
		//	收工签收机制
		acknowledgment.acknowledge();
	}
}

```
# 基本概念
```
 一个 broker 属于机器IP的进程(端口号) -> 多个TOPIC -> 一个topic可以有多个分区partition 
 
 生产者 -> leader的topic 
 
 消费者 可以消费多个分区
 
 一个分区只有一个消费者
```

# kafka配置

![](/images/kafka/消费3.png)

```
 一个topic可以有多个分区 partition
 
 一个分区只能由消费组的一个consumer消费
 
 partition 只能有一个 consumer
 
 consumer = partition  几乎相等分配
 
 consumer > partition  有闲置的consumer
 
 consumer < partition   一个consumer可以负责多个 partition
```

# 服务端必要参数

```
broker.id 必配参数,不得重复,取值0-n

zookeeper.connect 

ZooKeeper 连接字符串的格式为：hosts：hostname1:port1, hostname2:port2, hostname3:port3.

log.dirs : 不要使用默认值
```

# 服务端推荐参数

```
advertised.host.name  作为 broker 的 hostname 发往 producer、consumers 以及其他 brokers

advertised.port 注册到ZK供用户使用的服务端口

num.partitions 创建topic默认分区数量 默认1

default.replication.factor 自动创建topic的默认副本数量,建议至少修改为2

min.insync.replicasISR 提交生成者请求的最小副本数,至少2~3个

unclean.leader.election.enable 是否允许不具备ISR资格的replicas被选举为leader | 否

controlled.shutdown.enable  在kafka收到stop命令或异常终止时,允许自动同步数据,建议开启
```

# 动态调整参数

```
unclean.leader.election.enable 不严格的leader选举,有助于集群健壮,但是UC你在数据丢失风险

min.insync.replicas 如果同步状态的副本小于该值服务器将不在接收request.required.acks为-1或all的写入请求

max.message.bytes 单条消息的最大长度 如果修改了该值那么replica.fetch.max.bytes和消费者的fetch.message.max.bytes也要跟着修改

cleanup.policy 生命周期终结数据的处理,默认删除

flush.messages 强制刷新写入的最大缓存消息数

flush.ms 强制刷新写入的最大等待时长
```

# 客户端配置

```
Producer: ack,压缩,同步生产vs异步生产,批处理大小(异步生产)
Consumer:partition数量和获取消息的大小
```

# 服务最佳实践

```
JVM参数建议
    使用G1垃圾回收器
    -Xmx6g -Xms6g -XX:MetaspaceSize=96m -XX:+UseG1GC -XX:MaxGCPauseMillis=20
    -XX:InitiatingHeapOccupancyPercent=35 -XX:G1HeapRegionSize=16M
    -XX:MinMetaspaceFreeRatio=50 -XX:MaxMetaspaceFreeRatio=80 [ 24G内存四核英特尔至强CPUU 8x7200转SATA硬盘 ]
核心参数调整
    文件描述符数量调整:  (number_of_partitions)*(partition_size/segment_size)  100000以上
    最大套接字缓冲区大小: 
    pagecache 尽量分配大多数日志的激活日志段大小一致
    禁用swap
    设计broker的数量 单broker上的分区数<2000,分区大小不要超过25G
    消费组的consumer保持和partition一致
```

# Kafka基本命令

```
#生产者
bin/kafka-console-producer.sh --broker-list 172.20.132.87:9092 --topic test-stream-in

#消费者
bin/kafka-console-consumer.sh --bootstrap-server 172.20.132.87:9092 \
 --topic test-stream-out \
 --property print.key=true \
 --property print.value=true \
 --property key.deserializer=org.apache.kafka.common.serialization.StringDeserializer \
 --property value.deserializer=org.apache.kafka.common.serialization.LongDeserializer \
 --from-beginning

1、启动Kafka
bin/kafka-server-start.sh config/server.properties &

2、停止Kafka
bin/kafka-server-stop.sh

3、创建Topic
bin/kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic jiangzh-topic

4、查看已经创建的Topic信息
bin/kafka-topics.sh --list --zookeeper localhost:2181

5、发送消息
bin/kafka-console-producer.sh --broker-list 192.168.220.128:9092 --topic jiangzh-topic

6、接收消息
bin/kafka-console-consumer.sh --bootstrap-server 192.168.220.128:9092 --topic jiangzh-topic --from-beginning

# windows 中文乱码
chcp 65001

## 创建topic（4个分区，2个副本）
bin/kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 2 --partitions 4 --topic test

### kafka版本 >= 2.2
bin/kafka-topics.sh --create --bootstrap-server localhost:9092 --replication-factor 1 --partitions 1 --topic test

## 分区扩容
### kafka版本 < 2.2
bin/kafka-topics.sh --zookeeper localhost:2181 --alter --topic topic1 --partitions 2

### kafka版本 >= 2.2
bin/kafka-topics.sh --bootstrap-server broker_host:port --alter --topic topic1 --partitions 2

## 删除topic
bin/kafka-topics.sh --zookeeper localhost:2181 --delete --topic test

## 查询集群描述
bin/kafka-topics.sh --describe --zookeeper 127.0.0.1:2181

## 查询集群描述（新）
bin/kafka-topics.sh --bootstrap-server localhost:9092 --topic foo --describe

## topic列表查询
bin/kafka-topics.sh --zookeeper 127.0.0.1:2181 --list

## topic列表查询（支持0.9版本+）
bin/kafka-topics.sh --list --bootstrap-server localhost:9092

## 消费者列表查询（存储在zk中的）
bin/kafka-consumer-groups.sh --zookeeper localhost:2181 --list

## 消费者列表查询（支持0.9版本+）
bin/kafka-consumer-groups.sh --new-consumer --bootstrap-server localhost:9092 --list

## 消费者列表查询（支持0.10版本+）
bin/kafka-consumer-groups.sh --bootstrap-server localhost:9092 --list

## 显示某个消费组的消费详情（仅支持offset存储在zookeeper上的）
bin/kafka-run-class.sh kafka.tools.ConsumerOffsetChecker --zookeeper localhost:2181 --group test

## 显示某个消费组的消费详情（0.9版本 - 0.10.1.0 之前）
bin/kafka-consumer-groups.sh --new-consumer --bootstrap-server localhost:9092 --describe --group test-consumer-group

## 显示某个消费组的消费详情（0.10.1.0版本+）
bin/kafka-consumer-groups.sh --bootstrap-server localhost:9092 --describe --group my-group

## 生产者
bin/kafka-console-producer.sh --broker-list localhost:9092 --topic test

## 消费者
bin/kafka-console-consumer.sh --zookeeper localhost:2181 --topic test

## 生产者（支持0.9版本+）
bin/kafka-console-producer.sh --broker-list localhost:9092 --topic test --producer.config config/producer.properties

## 消费者（支持0.9版本+）
bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic test --new-consumer --from-beginning --consumer.config config/consumer.properties

## 消费者（最新）
bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic test --from-beginning --consumer.config config/consumer.properties

## kafka-verifiable-consumer.sh（消费者事件，例如：offset提交等）
bin/kafka-verifiable-consumer.sh --broker-list localhost:9092 --topic test --group-id groupName

## 高级点的用法
bin/kafka-simple-consumer-shell.sh --brist localhost:9092 --topic test --partition 0 --offset 1234  --max-messages 10

## kafka持续发送消息
kafka-verifiable-producer.sh --broker-list $(hostname -i):9092 --topic test --max-messages 100000

## kafka自带压测命令
bin/kafka-producer-perf-test.sh --topic test --num-records 100 --record-size 1 --throughput 100  --producer-props bootstrap.servers=localhost:9092

## 切换leader
## kafka版本 <= 2.4
> bin/kafka-preferred-replica-election.sh --zookeeper zk_host:port/chroot

## kafka新版本
> bin/kafka-preferred-replica-election.sh --bootstrap-server broker_host:port

##删除消费者组
kafka-consumer-groups.sh --bootstrap-server {Kafka instance connection address} --delete --group {consumer group name}

# 监听消费者
./kafka-console-consumer.bat  --bootstrap-server kafka:9092 --from-beginning --topic cent-order-record-add

# 查看topic
./kafka-topics.bat -zookeeper zookeeper:2181 -describe -topic cent-order-record-add 

# 所有topic
./kafka-topics.bat  --zookeeper zookeeper:2181 --list

#创建topic
./kafka-topics.bat --create --zookeeper zookeeper:2181 --replication-factor 3 --partitions 3 --topic order-test
```

# kafka消费的两种方式

![](/images/kafka/消费1.png)

```
public class ConsumerThreadSample {
    private final static String TOPIC_NAME = "test-topic";
    private final static String kafkaId = "172.20.132.87:9092";
    /*
     * 这种类型是经典模式，每一个线程单独创建一个KafkaConsumer，用于保证线程安全
     */
    public static void main(String[] args) throws InterruptedException {
        KafkaConsumerRunner r1 = new KafkaConsumerRunner();
        Thread t1 = new Thread(r1);
        t1.start();
        Thread.sleep(15000);
        r1.shutdown();
    }
    public static class KafkaConsumerRunner implements Runnable{
        private final AtomicBoolean closed = new AtomicBoolean(false);
        private final KafkaConsumer consumer;
        public KafkaConsumerRunner() {
            Properties props = new Properties();
            props.put("bootstrap.servers",kafkaId);
            props.put("group.id", "test");
            props.put("enable.auto.commit", "false");
            props.put("auto.commit.interval.ms", "1000");
            props.put("session.timeout.ms", "30000");
            props.put("key.deserializer", "org.apache.kafka.common.serialization.StringDeserializer");
            props.put("value.deserializer", "org.apache.kafka.common.serialization.StringDeserializer");
            consumer = new KafkaConsumer<>(props);
            TopicPartition p0 = new TopicPartition(TOPIC_NAME, 0);
            TopicPartition p1 = new TopicPartition(TOPIC_NAME, 1);
            consumer.assign(Arrays.asList(p0,p1));
        }
        @Override
        public void run() {
            try {
                while(!closed.get()) {
                    //处理消息
                    ConsumerRecords<String, String> records = consumer.poll(Duration.ofMillis(10000));

                    for (TopicPartition partition : records.partitions()) {
                        List<ConsumerRecord<String, String>> pRecord = records.records(partition);
                        // 处理每个分区的消息
                        for (ConsumerRecord<String, String> record : pRecord) {
                            System.out.printf("patition = %d , offset = %d, key = %s, value = %s%n",
                                    record.partition(),record.offset(), record.key(), record.value());
                        }

                        // 返回去告诉kafka新的offset
                        long lastOffset = pRecord.get(pRecord.size() - 1).offset();
                        // 注意加1
                        consumer.commitSync(Collections.singletonMap(partition, new OffsetAndMetadata(lastOffset + 1)));
                    }

                }
            }catch(WakeupException e) {
                if(!closed.get()) {
                    throw e;
                }
            }finally {
                consumer.close();
            }
        }

        public void shutdown() {
            closed.set(true);
            consumer.wakeup();
        }
    }
}
```

![](/images/kafka/消费2.png)

```
public class ConsumerRecordThreadSample {
    private final static String TOPIC_NAME = "test-topic";
    private final static String kafkaId = "172.20.132.87:9092";

    public static void main(String[] args) throws InterruptedException {
        String brokerList = kafkaId;
        String groupId = "test";
        int workerNum = 5;
        CunsumerExecutor consumers = new CunsumerExecutor(brokerList, groupId, TOPIC_NAME);
        consumers.execute(workerNum);
        Thread.sleep(1000000);
        consumers.shutdown();
    }

    // Consumer处理
    public static class CunsumerExecutor{
        private final KafkaConsumer<String, String> consumer;
        private ExecutorService executors;
        public CunsumerExecutor(String brokerList, String groupId, String topic) {
            Properties props = new Properties();
            props.put("bootstrap.servers", brokerList);
            props.put("group.id", groupId);
            props.put("enable.auto.commit", "true");
            props.put("auto.commit.interval.ms", "1000");
            props.put("session.timeout.ms", "30000");
            props.put("key.deserializer", "org.apache.kafka.common.serialization.StringDeserializer");
            props.put("value.deserializer", "org.apache.kafka.common.serialization.StringDeserializer");
            consumer = new KafkaConsumer<>(props);
            consumer.subscribe(Arrays.asList(topic));
        }
        public void execute(int workerNum) {
            executors = new ThreadPoolExecutor(workerNum, workerNum, 0L, TimeUnit.MILLISECONDS,
                    new ArrayBlockingQueue<>(1000), new ThreadPoolExecutor.CallerRunsPolicy());
            while (true) {
                ConsumerRecords<String, String> records = consumer.poll(200);
                for (final ConsumerRecord record : records) {
                    executors.submit(new ConsumerRecordWorker(record));
                }
            }
        }
        public void shutdown() {
            if (consumer != null) {
                consumer.close();
            }
            if (executors != null) {
                executors.shutdown();
            }
            try {
                if (!executors.awaitTermination(10, TimeUnit.SECONDS)) {
                    System.out.println("Timeout.... Ignore for this case");
                }
            } catch (InterruptedException ignored) {
                System.out.println("Other thread interrupted this shutdown, ignore for this case.");
                Thread.currentThread().interrupt();
            }
        }
    }
    // 记录处理
    public static class ConsumerRecordWorker implements Runnable {
        private ConsumerRecord<String, String> record;
        public ConsumerRecordWorker(ConsumerRecord record) {
            this.record = record;
        }
        @Override
        public void run() {
            // 假如说数据入库操作
            System.out.println("Thread - "+ Thread.currentThread().getName());
            System.err.printf("patition = %d , offset = %d, key = %s, value = %s%n",
                    record.partition(), record.offset(), record.key(), record.value());
        }
    }
}
```

# 手动控制消费

```
// 手动指定offset起始位置

    1、人为控制offset起始位置
    2、如果出现程序错误，重复消费一次
---------------------------------------------------
    1、第一次从0消费【一般情况】
    2、比如一次消费了100条， offset置为101并且存入Redis
    3、每次poll之前，从redis中获取最新的offset位置
    4、每次从这个位置开始消费
 */
consumer.seek(p0, 700);
```
# 限流

```
private static void controlPause() {
    Properties props = new Properties();
    props.setProperty("bootstrap.servers",kafkaId);
    props.setProperty("group.id", "test");
    props.setProperty("enable.auto.commit", "false");
    props.setProperty("auto.commit.interval.ms", "1000");
    props.setProperty("key.deserializer", "org.apache.kafka.common.serialization.StringDeserializer");
    props.setProperty("value.deserializer", "org.apache.kafka.common.serialization.StringDeserializer");

    KafkaConsumer<String, String> consumer = new KafkaConsumer(props);

    // jiangzh-topic - 0,1两个partition
    TopicPartition p0 = new TopicPartition(TOPIC_NAME, 0);
    TopicPartition p1 = new TopicPartition(TOPIC_NAME, 1);

    // 消费订阅某个Topic的某个分区
    consumer.assign(Arrays.asList(p0,p1));
    long totalNum = 40;
    while (true) {
        ConsumerRecords<String, String> records = consumer.poll(Duration.ofMillis(10000));
        // 每个partition单独处理
        for(TopicPartition partition : records.partitions()){
            List<ConsumerRecord<String, String>> pRecord = records.records(partition);
            long num = 0;
            for (ConsumerRecord<String, String> record : pRecord) {
                System.out.printf("patition = %d , offset = %d, key = %s, value = %s%n",
                        record.partition(), record.offset(), record.key(), record.value());
                /*
                    1、接收到record信息以后，去令牌桶中拿取令牌
                    2、如果获取到令牌，则继续业务处理
                    3、如果获取不到令牌， 则pause等待令牌
                    4、当令牌桶中的令牌足够， 则将consumer置为resume状态
                 */
                num++;
                if(record.partition() == 0){
                    if(num >= totalNum){
                        consumer.pause(Arrays.asList(p0));
                    }
                }

                if(record.partition() == 1){
                    if(num == 40){
                        consumer.resume(Arrays.asList(p0));
                    }
                }
            }

            long lastOffset = pRecord.get(pRecord.size() -1).offset();
            // 单个partition中的offset，并且进行提交
            Map<TopicPartition, OffsetAndMetadata> offset = new HashMap<>();
            offset.put(partition,new OffsetAndMetadata(lastOffset+1));
            // 提交offset
            consumer.commitSync(offset);
            System.out.println("=============partition - "+ partition +" end================");
        }
    }
}
```

# 新消费者 - 加入-崩溃-离组

* 类似于乐观锁,添加会有版本号的概念

![](/images/kafka/消费-加入.png)

![](/images/kafka/消费-崩溃.png)

![](/images/kafka/消费-离组.png)

* 离组的 位移值会有脏数据 - 新版解决方案类似于jvm的stop-all-world

![](/images/kafka/消费-离组-位移.png)

# kafka实时流计算

```
public class StreamSample {
    
    public static void main(String[] args) {
        Properties props = new Properties();
        props.put(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, EnumUtil.KAFKA_ID);
        props.put(StreamsConfig.APPLICATION_ID_CONFIG, "wordcount-app");
        props.put(StreamsConfig.DEFAULT_KEY_SERDE_CLASS_CONFIG, Serdes.String().getClass());
        props.put(StreamsConfig.DEFAULT_VALUE_SERDE_CLASS_CONFIG, Serdes.String().getClass());
        // 如果构建流结构拓扑
        final StreamsBuilder builder = new StreamsBuilder();
        // 构建Wordcount
        wordcountStream(builder);
        // 构建foreachStream
        //foreachStream(builder);
        final KafkaStreams streams = new KafkaStreams(builder.build(), props);
        streams.start();
    }
    // 如果定义流计算过程
    static void foreachStream(final StreamsBuilder builder) {
        KStream<String, String> source = builder.stream(EnumUtil.INPUT_TOPIC);
        source
                .flatMapValues(value -> Arrays.asList(value.toLowerCase(Locale.getDefault()).split(" ")))
                .foreach((key, value) -> System.out.println(key + " : " + value));
    }
    // 如果定义流计算过程
    static void wordcountStream(final StreamsBuilder builder) {
        // 不断从INPUT_TOPIC上获取新数据，并且追加到流上的一个抽象对象
        KStream<String, String> source = builder.stream(EnumUtil.INPUT_TOPIC);
        // Hello World imooc
        // KTable是数据集合的抽象对象
        // 算子
        final KTable<String, Long> count =
                source
                        // flatMapValues -> 将一行数据拆分为多行数据  key 1 , value Hello World
                        // flatMapValues -> 将一行数据拆分为多行数据  key 1 , value Hello key xx , value World
                        /*
                            key 1 , value Hello   -> Hello 1  World 2
                            key 2 , value World
                            key 3 , value World
                         */
                        .flatMapValues(value -> Arrays.asList(value.toLowerCase(Locale.getDefault()).split(" ")))
                        // 合并 -> 按value值合并
                        .groupBy((key, value) -> value)
                        // 统计出现的总数
                        .count();

        // 将结果输入到OUT_TOPIC中
        count.toStream().to(EnumUtil.OUT_TOPIC, Produced.with(Serdes.String(), Serdes.Long()));
    }
}
```

# kafka connection api
![](/images/kafka/connect.png)

```
# 配置kafka connection 集群基本配置
    vi connect-distributed.properties
    bootstrap.servers=172.20.132.87:9092
    plugin.path=/data/sort/plugins
    rest.port=8083
    
# 驱动网站
 https://www.confluent.io/connector/kafka-connect-jdbc/

## connect启动命令
bin/connect-distributed.sh -daemon config/connect-distributed.properties
bin/connect-distributed.sh config/connect-distributed.properties

## 检查
http://172.20.132.87:8083/connector-plugins
http://172.20.132.87:8083/connectors

#  mysql修改数据推动kafka
curl -X POST -H 'Content-Type: application/json' -i 'http://172.20.132.87:8083/connectors' \
--data \
'{"name":"imooc-upload-mysql","config":{
"connector.class":"io.confluent.connect.jdbc.JdbcSourceConnector",
"connection.url":"jdbc:mysql://172.20.132.87:3306/kafka_study?user=root&password=root",
"table.whitelist":"users",
"incrementing.column.name": "uuid",
"mode":"incrementing",
"topic.prefix": "imooc-mysql-"}}'
bin/kafka-console-consumer.sh --bootstrap-server 172.20.132.87:9092 --topic imooc-mysql-users --from-beginning

# kafka 通道插入数据库
curl -X POST -H 'Content-Type: application/json' -i 'http://172.20.132.87:8083/connectors' \
--data \
'{"name":"imooc-download-mysql","config":{
"connector.class":"io.confluent.connect.jdbc.JdbcSinkConnector",
"connection.url":"jdbc:mysql://172.20.132.87:3306/kafka_study?user=root&password=123456",
"topics":"imooc-mysql-users",
"auto.create":"false",
"insert.mode": "upsert",
"pk.mode":"record_value",
"pk.fields":"uuid",
"table.name.format": "users_bak"}}'

bin/kafka-console-consumer.sh --bootstrap-server 192.168.220.128:9092 --topic test-mysql-jdbc-users --from-beginning

# 删除通道
curl -X DELETE -i 'http://192.168.220.128:8083/connectors/load-mysql-data'
```

# kafka 集群

```
#部署
    复制多几份kafka
    vi config/server.properties
    broker.id 不能轻易修改 topic和次ID有绑定,如果修改了topic就会无法访问

    broker.id=0
    listeners = PLAINTEXT://172.20.132.87:9091
    advertised.listeners=PLAINTEXT://172.20.132.87:9091
    log.dirs=/tmp/kafka-logs-1
    zookeeper.connect=localhost:2181
    
# 核心概念
    Broker: 一般是指kafka的部署节点,
        集群是一个机器启动一个kafka端口, Broker是每一台机器的ip 
        单机是指 启动kafka的每一个进程
        
    Leader: 用于处理消息的接收和消费等请求
    Follower:主要用于备份消息数据

# kafka节点故障
    kafka 与 zookeeper 心跳未保持  大概30S
    follower 消息落后leader太多

# 节点故障处理
    kafka基本不会因为节点故障丢失数据
    kafka的语义担保保障数据不会丢失  0最多一次  1 至少一次 all 超过一半以上副本都响应了才确认发送成功了
    kafka会对消息进行集群被平衡,减少消息在某些节点热度过高

# kafka 集群之leader选举
    kafka并没有采用多数投票才选举leader
    kafka会动态维护一组leader数据的副本(isr)
    kafka会在isr中选择一个速度比较快的设为leader
    isr全部宕机,kafka会进行unclean leader选举
        isr死等,直到恢复
        选举isr以外的follower,会丢数据
        
# 选举配置建议    
    禁用unclean leader选举
    手动指定最小isr,isr小于两个就不发送消息,然后业务去控制异常

# kafka集群监控
    kafka只能依靠kafka-run-class.sh等命令来管理
    kafka Manager 监控管理工具(https://blog.51cto.com/liqingbiao/2417010)
    
    #启动
    修改 conf/ zookeeper 地址
    
    # bin/kafka-manager -Dhttp.port=8081
```

# kafka ssl机制

```
创建密钥仓库，用于存储证书文件
keytool -keystore server.keystore.jks -alias xxkafka -validity 100000 -genkey

创建CA
openssl req -new -x509 -keyout ca-key -out ca-cert -days 100000

将生成的CA添加到客户信任库
keytool -keystore client.truststore.jks -alias CARoot -import -file ca-cert

为broker提供信任库以及所有客户端签名了密钥的CA证书
keytool -keystore server.truststore.jks -alias CARoot -import -file ca-cert

签名证书，用自己生成的CA来签名前面生成的证书
1、从密钥仓库导出证书
keytool -keystore server.keystore.jks -alias xxkafka -certreq -file cert-file

2、用CA签名：
openssl x509 -req -CA ca-cert -CAkey ca-key -in cert-file -out cert-signed -days 100000 -CAcreateserial -passin pass:root123456

3、导入CA的证书和已签名的证书到密钥仓库
keytool -keystore server.keystore.jks -alias CARoot -import -file ca-cert
keytool -keystore server.keystore.jks -alias xxkafka -import -file cert-signed

# 配置
kafka server.properties:

	listeners=PLAINTEXT://172.20.132.87:9091,SSL://172.20.132.87:8989
	advertised.listeners=PLAINTEXT://172.20.132.87:9091,SSL://172.20.132.87:8989
	ssl.keystore.location=/opt/ca-tmp/server.keystore.jks
	ssl.keystore.password=root123456
	ssl.key.password=root123456
	ssl.truststore.location=/opt/ca-tmp/server.truststore.jks
	ssl.truststore.password=root123456

# 测试SSL是否成功
openssl s_client -debug -connect 172.20.132.87:8989 -tls1

# 客户端配置：
security.protocol=SSL
ssl.endpoint.identification.algorithm=
ssl.truststore.location=/opt/ca-tmp2/client.truststore.jks
ssl.truststore.password=root123456
```

# 案例 spring bus 动态配置

```
# springCloud config

    ## SpringCloud
    <dependencyManagement>
        <dependencies>
            <dependency>
                <groupId>org.springframework.cloud</groupId>
                <artifactId>spring-cloud-dependencies</artifactId>
                <version>Hoxton.SR4</version>
                <type>pom</type>
                <scope>import</scope>
            </dependency>
        </dependencies>
    </dependencyManagement>
    
    ## Config Server
    <dependency>
        <groupId>org.springframework.cloud</groupId>
        <artifactId>spring-cloud-config-server</artifactId>
    </dependency>
    
    spring.application.name=config-server
    server.port=8900
    spring.cloud.config.server.git.uri = xxx
    spring.cloud.config.label=master
    spring.cloud.config.server.git.username= xx
    spring.cloud.config.server.git.password= xx
    
    ## 验证config server
    http://localhost:8900/kafka/master

    ## Config Client
    <dependency>
        <groupId>org.springframework.boot</groupId>
        <artifactId>spring-boot-starter-web</artifactId>
    </dependency>
    <dependency>
        <groupId>org.springframework.cloud</groupId>
        <artifactId>spring-cloud-starter-config</artifactId>
    </dependency>
    
    ## bootstrap.yml配置：
    spring:
        application:
            name: kafka
        cloud:
            config:
                uri: http://localhost:8900/
                label: master
 
    ## 动态刷新配置依赖
    <dependency>
        <groupId>org.springframework.boot</groupId>
        <artifactId>spring-boot-starter-actuator</artifactId>
    </dependency>
    
    ## 增加访问point
    management.endpoints.web.exposure.include=health, info, refresh
    
    ## post请求访问/actuator/refresh
    curl -XPOST http://localhost:7002/actuator/refresh

# springCloud bus
    ## 配置依赖：
    <dependency>
        <groupId>org.springframework.cloud</groupId>
        <artifactId>spring-cloud-starter-bus-kafka</artifactId>
    </dependency>
    
    ## Kafka配置：
    
    spring.cloud.stream.kafka.binder.zkNodes=xx:2181
    spring.cloud.stream.kafka.binder.brokers=xx:9092
    
    management:
      endpoints:
        web:
          exposure:
            include: health, info, refresh, bus-refresh
    
    ## 配置刷新
    curl -X POST http://localhost:7002/actuator/bus-refresh

    ## 测试：
    http://localhost:7001/test
    http://localhost:7002/test
```

# 配置大全

[kafka.pdf](/resources/kafka配置.pdf)

# kafka 面试点

## 总览
* 日志格式
![](/images/kafka/消息.png)
![](/images/kafka/log.png)

* 零拷贝
![](/images/kafka/zerofile1.png)
![](/images/kafka/zerofile2.png)

```
1.为什么使用kafka / kafka和其他中间件不同 / kafka好在哪里
    kafka是一个分布式流处理平台,值是提供发布订阅及Topic支持,所以才会和其他MQ作比较
    kafka吞吐量高但不保证消息有序,单partition内有序
    kafka提供了offest管理,历史消息也可以再次消费
    kafka为了高效性,数据没有同步刷盘,所以可能存在丢数据的情况
2. 场景使用
    日志收集和流式系统
    消息系统
    用户活动跟踪或运营指标监控
3.kafka 为什么吞吐量大?
    生产者 -> 落磁盘 ->消费者 -> 读磁盘
    关于IO的优化
        日志顺序读写和快速检索
        partition机制 - 一个topic分为多个partition ,可有效分配热点数据,减少数据积压
        批量发送接收和数据压缩机制(减少网络传输) - 批量数据压缩比单条数据效率高
        通过sendFile实现零拷贝原则
       
4.日志顺序读写和快速检索是什么?
    kafka的日志是以partition为单位进行保存
    日志目录是以topic名称+数字
    日志文件格式是一个"日志条目"序列
    每条日志消息由4字节整形与N字节消息组成
    日志分段
        每个partition的日志会分为N个大小相等的segment中
        每个segment中消息数量不一定相等
        每个partition只支持顺序读写
     segment存储结构
        partition会将消息添加到最后一个segment上
        当segment达到一定阈值会flush到磁盘上
        segment文件会被划分成index,log两部分 
    日志读操作(逐级检索)
        首先需要在存储的数据中找出segment文件
        然后通过全局offset计算出segment中的offset
        通过index中的offset寻找具体数据内容
    日志写操作
        日志允许串行的追加到消息文件最后
        当日志文件达到阈值则滚动到新文件上 
5.sendfile 零拷贝
    抛弃用户缓冲区,直接在内核读取文件

     
```
## 生产消息
* producer 创建两个线程 核心线程+守护线程
* 守护线程:时间或者size阈值批量推送,并回调future
  

![](/images/kafka/producer.png)
![](/images/kafka/producer1.png)

## kafka保证有序性
```
1.单topic 单partition
2.使用kafka key+offset可以做到业务有序
    订单号作为key offset的先后顺序来标记该订单的业务顺序
```

## kafka topic删除

![](/images/kafka/del.png)

```
* 通过异步线程删除,
* 尽量将kafka停掉再删除
* auto.create.topics.enable = false
* delete.topic.enable = true 
```

# Rabbit MQ
![](/images/mq/MQ一致性.png)
```
1.4种模式: 
  主备模式(主从)
  远程模式->远距离的复制,不同中心的复制和转移
  镜像模式->内部数据节点数据备份,奇数个节点
  多活模型集群->镜像模式+远程模式,通过插件同步数据
2.AMQP协议
  publiser
  |
  server (broker)
    Virtual host
    |
    Exchange
    | 绑定多个队列
    MSG queue
  |
  client
3.@RabbitMQListener  @QueueBinding @Exchange @Queue
  @RabbitHandler 获取消息
4.生产者
import org.springframework.amqp.AmqpException;
import org.springframework.amqp.core.MessagePostProcessor;
import org.springframework.amqp.rabbit.connection.CorrelationData;
import org.springframework.amqp.rabbit.core.RabbitTemplate;
import org.springframework.beans.factory.annotation.Autowired;
import org.springframework.messaging.Message;
import org.springframework.messaging.MessageHeaders;
import org.springframework.messaging.support.MessageBuilder;
import org.springframework.stereotype.Component;
import java.util.Map;
import java.util.UUID;
@Component
public class RabbitSender {
	@Autowired
	RabbitTemplate rabbitTemplate;
	/**
	 * 这里是确认消息的回调监听接口,用于确认消息是否被broker收到
	 */
	final RabbitTemplate.ConfirmCallback confirmCallback = new RabbitTemplate.ConfirmCallback() {
		/**
		 * correlationData  作为一个唯一标识
		 * ack 是否落盘成功
		 * cause 失败的异常原因
		 */
		@Override
		public void confirm(CorrelationData correlationData, boolean ack, String cause) {
			System.err.println("!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!消息ACK:" + ack + ",correlationData" + correlationData.getId() + "," + cause);
		}
	};
	public void send(Object message, Map<String, Object> properties) throws Exception {
		MessageHeaders messageHeaders = new MessageHeaders(properties);
		Message<Object> msg = MessageBuilder.createMessage(message, messageHeaders);
		rabbitTemplate.setConfirmCallback(confirmCallback);
		CorrelationData correlationData = new CorrelationData(UUID.randomUUID().toString());
		rabbitTemplate.convertAndSend("exchange-1", "springboot.rabbit", msg, new MessagePostProcessor() {
			@Override
			public org.springframework.amqp.core.Message postProcessMessage(org.springframework.amqp.core.Message message) throws AmqpException {
				System.err.println("!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!post todo :" + message);
				return message;
			}
		}, correlationData);
	}
}
5.消费者
import com.rabbitmq.client.Channel;
import org.springframework.amqp.rabbit.annotation.Exchange;
import org.springframework.amqp.rabbit.annotation.Queue;
import org.springframework.amqp.rabbit.annotation.QueueBinding;
import org.springframework.amqp.rabbit.annotation.RabbitHandler;
import org.springframework.amqp.rabbit.annotation.RabbitListener;
import org.springframework.amqp.rabbit.core.RabbitTemplate;
import org.springframework.amqp.support.AmqpHeaders;
import org.springframework.beans.factory.annotation.Autowired;
import org.springframework.messaging.Message;
import org.springframework.stereotype.Component;

@Component
public class RabbitCumsuer {

	@Autowired
	RabbitTemplate rabbitTemplate;
	
	@RabbitListener(
			bindings = @QueueBinding(
					value = @Queue(value = "queue-1", declare = "true"),
					exchange = @Exchange(
							name = "exchange-1",
							durable = "true",
							type = "topic",
							ignoreDeclarationExceptions = "true"),
					key = "springboot.*"
			)
	)
	@RabbitHandler
	public void onMessage(Message message, Channel channel) throws Exception {
		System.err.println("消费消息:" + message.getPayload());
		Long deliveryTag = (Long) message.getHeaders().get(AmqpHeaders.DELIVERY_TAG);
		channel.basicAck(deliveryTag, false);
	}
}  
```

# 池化操作
```
package com.bfxy.rabbit.producer.broker;

import java.util.List;
import java.util.Map;

import org.springframework.amqp.rabbit.connection.ConnectionFactory;
import org.springframework.amqp.rabbit.connection.CorrelationData;
import org.springframework.amqp.rabbit.core.RabbitTemplate;
import org.springframework.beans.factory.annotation.Autowired;
import org.springframework.retry.support.RetryTemplate;
import org.springframework.stereotype.Component;

import com.bfxy.rabbit.api.Message;
import com.bfxy.rabbit.api.MessageType;
import com.bfxy.rabbit.api.exception.MessageRunTimeException;
import com.bfxy.rabbit.common.convert.GenericMessageConverter;
import com.bfxy.rabbit.common.convert.RabbitMessageConverter;
import com.bfxy.rabbit.common.serializer.Serializer;
import com.bfxy.rabbit.common.serializer.SerializerFactory;
import com.bfxy.rabbit.common.serializer.impl.JacksonSerializerFactory;
import com.bfxy.rabbit.producer.service.MessageStoreService;
import com.google.common.base.Preconditions;
import com.google.common.base.Splitter;
import com.google.common.collect.Maps;

import lombok.extern.slf4j.Slf4j;

/**
 * 	$RabbitTemplateContainer池化封装
 * 	每一个topic 对应一个RabbitTemplate
 *	1.	提高发送的效率
 * 	2. 	可以根据不同的需求制定化不同的RabbitTemplate, 比如每一个topic 都有自己的routingKey规则
 * @author Alienware
 */
@Slf4j
@Component
public class RabbitTemplateContainer implements RabbitTemplate.ConfirmCallback {

	private Map<String /* TOPIC */, RabbitTemplate> rabbitMap = Maps.newConcurrentMap();

	private Splitter splitter = Splitter.on("#");

	private SerializerFactory serializerFactory = JacksonSerializerFactory.INSTANCE;

	@Autowired
	private ConnectionFactory connectionFactory;

	@Autowired
	private MessageStoreService messageStoreService;

	public RabbitTemplate getTemplate(Message message) throws MessageRunTimeException {
		Preconditions.checkNotNull(message);
		String topic = message.getTopic();
		RabbitTemplate rabbitTemplate = rabbitMap.get(topic);
		if(rabbitTemplate != null) {
			return rabbitTemplate;
		}
		log.info("#RabbitTemplateContainer.getTemplate# topic: {} is not exists, create one", topic);

		RabbitTemplate newTemplate = new RabbitTemplate(connectionFactory);
		newTemplate.setExchange(topic);
		newTemplate.setRoutingKey(message.getRoutingKey());
		newTemplate.setRetryTemplate(new RetryTemplate());

		//	添加序列化反序列化和converter对象
		Serializer serializer = serializerFactory.create();
		GenericMessageConverter gmc = new GenericMessageConverter(serializer);
		RabbitMessageConverter rmc = new RabbitMessageConverter(gmc);
		newTemplate.setMessageConverter(rmc);

		String messageType = message.getMessageType();
		if(!MessageType.RAPID.equals(messageType)) {
			//只要不是迅速消息,需要确认就设置callback
			// 将每一个的callback回调放在template的池子里面-- 点睛之笔
			newTemplate.setConfirmCallback(this);
		}

		rabbitMap.putIfAbsent(topic, newTemplate);

		return rabbitMap.get(topic);
	}

	/**
	 * 	无论是 confirm 消息 还是 reliant 消息 ，发送消息以后 broker都会去回调confirm
	 */
	@Override
	public void confirm(CorrelationData correlationData, boolean ack, String cause) {
		// 	具体的消息应答
		List<String> strings = splitter.splitToList(correlationData.getId());
		String messageId = strings.get(0);
		long sendTime = Long.parseLong(strings.get(1));
		String messageType = strings.get(2);
		if(ack) {
			//	当Broker 返回ACK成功时, 就是更新一下日志表里对应的消息发送状态为 SEND_OK
			// 	如果当前消息类型为reliant 我们就去数据库查找并进行更新
			if(MessageType.RELIANT.endsWith(messageType)) {
				this.messageStoreService.succuess(messageId);
			}
			log.info("send message is OK, confirm messageId: {}, sendTime: {}", messageId, sendTime);
		} else {
			log.error("send message is Fail, confirm messageId: {}, sendTime: {}", messageId, sendTime);

		}
	}
}


```

# docker安装

```
version: '3'

services:
  zk:
    image: wurstmeister/zookeeper
    container_name: zk
    hostname: zk
    ports:
      - "2181:2181"
  kafka:
    image: wurstmeister/kafka
    container_name: kafka
    depends_on:
      - zk
    hostname: kafka
    ports:
      - 9092:9092
    environment:
      - KAFKA_BROKER_ID=0
      - KAFKA_ZOOKEEPER_CONNECT=zk:2181
      - ALLOW_PLAINTEXT_LISTENER=yes
      - KAFKA_ADVERTISED_LISTENERS=PLAINTEXT://192.168.137.104:9092 # 外部容器的地址
      - KAFKA_LISTENERS=PLAINTEXT://0.0.0.0:9092
  kafdrop:
    image: obsidiandynamics/kafdrop:latest
    container_name: kafdrop
    depends_on:
      - kafka
    ports:
      - 9000:9000
    environment:
      - KAFKA_BROKERCONNECT=kafka:9092
# 验证
#e:
#cd  JJDK/kafka/bin/windows
#kafka-console-producer.bat --broker-list bigdata04:9092 --topic campus-user-info-input-test
#kafka-console-consumer.bat --bootstrap-server bigdata04:9092 --topic campus-user-info-input-test --from-#beginning
```
