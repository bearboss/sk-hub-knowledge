---
typora-root-url: ..\
---

title: Docker-464

date: 2021-05-25 15:20:37

tags: Docker

categories: Docker

copyright: true

sticky: 0

---

<span id="delete">

![](/images/banner/4.jpg)

</span>

<!--more-->

![](/images/docker/464/1.png)

# 基础
```
kubectl get pod -A
kubectl get deployment -A

kubectl cp ./dup_company_info.json dolphinscheduler-worker-0:/data/datax/job/org/ -n prod

kubectl run 资源名称 -参数 --image=镜像名称:标签	创建资源对象，常用参数-i交互，-t终端
kubectl get 查询资源 可选参数 -o wide 显示主机信息	常用查询的资源 node|deployment|pod
kubectl exec -it 容器id 执行的命令	同 docker exec 指令，进入容器内
kubectl describe 资源类型 资源名称	查询资源的详细信息
kubectl attach	同 docker attach 指令，连接容器
kubectl logs 容器id	查看容器控制台的标准输出
kubectl delete 资源类型 资源名称	删除指定的资源
kubectl create|apply -f 资源文件	执行指定的资源文件

#docker是通过clone实现命名空间的隔离
	unshare --fork --pid --mount-proc bash
	ps aux
#资源配额 CGroups 限制进程
	#原生限额
		while : ; do : ; done &
		cd /sys/fs/cgroup/cpu
		mkdir cgroups_test
		echo 20000 > /sys/fs/cgroup/cpu/cgroups_test/cpu.cfs_quota_us
		echo PID > /sys/fs/cgroup/cpu/cgroups_test/tasks
	#docker限额
		docker run -it --cpus=".5" nginx /bin/sh
		cd /sys/fs/cgroup/cpu
		cat cpu.cfs_quota_us
```

# 安装
```
#改名
	hostnamectl set-hostname node1
	vi /etc/'hostname
#时间同步
	yum install chrony -y
	systemctl start chronyd
	systemctl enable chronyd
	chronyc sources
# 配置源
	curl -o /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-7.repo
	yum clean all
	yum repolist
	yum install bash‐comp* vim net‐tools wget ‐y
# 配置源
	cat >>/etc/yum.repos.d/kubernetes.repo <<EOF
	[kubernetes]
	name=Kubernetes
	baseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64/
	enabled=1
	gpgcheck=1
	repo_gpgcheck=1
	gpgkey=https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg https://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg
	EOF
# 安装最新版docker
	--- 参考4.7docker的基础命令
#配置网络流量
	modprobe br_netfilter
	
	echo "1" >/proc/sys/net/bridge/bridge-nf-call-iptables
	vi /etc/sysctl.d/k8s.conf
		net.bridge.bridge-nf-call-ip6tables = 1
		net.bridge.bridge-nf-call-iptables = 1
	
	cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf
	br_netfilter
	EOF
	
	cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
	net.bridge.bridge-nf-call-ip6tables = 1
	net.bridge.bridge-nf-call-iptables = 1
	EOF
	
	sudo sysctl --system

	#第二种
		sudo modprobe br_netfilter
		lsmod | grep br_netfilter

		cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf
		overlay
		br_netfilter
		EOF
		
		sudo modprobe overlay
		sudo modprobe br_netfilter
		
		# 设置所需的 sysctl 参数，参数在重新启动后保持不变
		cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
		net.bridge.bridge-nf-call-iptables  = 1
		net.bridge.bridge-nf-call-ip6tables = 1
		net.ipv4.ip_forward                 = 1
		EOF
		
		# 应用 sysctl 参数而不重新启动
		sudo sysctl --system
		
#防火墙
	systemctl stop firewalld
	systemctl disable firewalld
	setenforce 0
	sed -i "s/SELINUX=enforcing/SELINUX=disabled/g" /etc/selinux/config
	# 临时关闭
	setenforce 0
	# 永久禁用
	sed -i 's/^SELINUX=enforcing$/SELINUX=disabled/' /etc/selinux/config
#禁用交换
	# 临时关闭；关闭swap主要是为了性能考虑
	swapoff -a
	# 可以通过这个命令查看swap是否关闭了
	free
	# 永久关闭
	sed -ri 's/.*swap.*/#&/' /etc/fstab
	#其他关闭
	yes | cp /etc/fstab /etc/fstab_bak
	cat /etc/fstab_bak |grep -v swap > /etc/fstab
	vi /etc/fstab
	注释 #/dev/mapper/centos-home /home                   xfs     defaults        0 0
	
#修改容器的配置(所有节点都需要修改)
		containerd config default > /etc/containerd/config.toml
		grep sandbox_image  /etc/containerd/config.toml
		sed -i "s#k8s.gcr.io/pause#registry.aliyuncs.com/google_containers/pause#g" /etc/containerd/config.toml
		grep sandbox_image  /etc/containerd/config.toml
		
		不行就手动修改 registry.aliyuncs.com/google_containers/pause
		systemctl restart containerd
		
#安装kube
	yum install -y kubelet kubeadm kubectl
	systemctl enable kubelet && systemctl start kubelet

#master初始化

  #查看所需要的镜像,提前下载
		kubeadm config images list 
		
		docker pull registry.cn-hangzhou.aliyuncs.com/google_containers/kube-apiserver:v1.27.1
		docker pull registry.cn-hangzhou.aliyuncs.com/google_containers/kube-controller-manager:v1.27.1
		docker pull registry.cn-hangzhou.aliyuncs.com/google_containers/kube-scheduler:v1.27.1
		docker pull registry.cn-hangzhou.aliyuncs.com/google_containers/kube-proxy:v1.27.1
		docker pull registry.cn-hangzhou.aliyuncs.com/google_containers/pause:3.9
		docker pull registry.cn-hangzhou.aliyuncs.com/google_containers/etcd:3.5.7-0
		docker pull registry.cn-hangzhou.aliyuncs.com/google_containers/coredns:v1.10.1
		
		docker tag registry.cn-hangzhou.aliyuncs.com/google_containers/kube-apiserver:v1.27.1 registry.k8s.io/kube-apiserver:v1.27.1
		docker tag registry.cn-hangzhou.aliyuncs.com/google_containers/kube-controller-manager:v1.27.1 registry.k8s.io/kube-controller-manager:v1.27.1
		docker tag registry.cn-hangzhou.aliyuncs.com/google_containers/kube-scheduler:v1.27.1 registry.k8s.io/kube-scheduler:v1.27.1
		docker tag registry.cn-hangzhou.aliyuncs.com/google_containers/kube-proxy:v1.27.1 registry.k8s.io/kube-proxy:v1.27.1
		docker tag registry.cn-hangzhou.aliyuncs.com/google_containers/pause:3.9 registry.k8s.io/pause:3.9
		docker tag registry.cn-hangzhou.aliyuncs.com/google_containers/etcd:3.5.7-0  registry.k8s.io/etcd:3.5.7-0
		docker tag registry.cn-hangzhou.aliyuncs.com/google_containers/coredns:v1.10.1 registry.k8s.io/coredns/coredns:v1.10.1

	#kube 初始化
	
		kubeadm init --kubernetes-version=1.27.1 \
		--apiserver-advertise-address=192.168.137.101 \
		--image-repository registry.aliyuncs.com/google_containers \
		--service-cidr=10.1.0.0/16 \
		--pod-network-cidr=10.244.0.0/16
		
		报错查看日志
		crictl --runtime-endpoint unix:///var/run/containerd/containerd.sock ps -a
		crictl --runtime-endpoint unix:///var/run/containerd/containerd.sock logs 6eb139f931ab8

		重置
		kubeadm reset -f
		rm -fr ~/.kube/  /etc/kubernetes/* var/lib/etcd/* /var/lib/kubelet/pki/*
		
		初始化成功之后
		mkdir -p $HOME/.kube
		sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
		sudo chown $(id -u):$(id -g) $HOME/.kube/config
		export KUBECONFIG=/etc/kubernetes/admin.conf
		
		永久生效(所有节点)
			echo "export KUBECONFIG=/etc/kubernetes/admin.conf" >> ~/.bash_profile
			source  ~/.bash_profile
		
  	- 安装 flannel 网络插件(所有节点) - node需要加入之后再执行
				kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
				ifconfig |grep flannel
		安装验证		
		kubectl get nodes 查看 node 节点处于ready状态
				
		重新生成token
			kubeadm token create --print-join-command

  	节点加入
			- 将 master 节点的 admin.conf 拷贝到 node1
				scp /etc/kubernetes/admin.conf root@node1:/etc/kubernetes/
			在node1节点上面创建目录：
				mkdir -p /etc/cni/net.d/
			在master： 
				scp /etc/cni/net.d/* root@node1:/etc/cni/net.d/
```

# 取别名&补全

```
apt-get install bash-completion
echo 'source <(kubectl completion bash)' >>~/.bashrc
kubectl completion bash >/etc/bash_completion.d/kubectl
```

```
yum install -y bash-completion bash-completion-extras

source /usr/share/bash-completion/bash_completion
kubectl completion bash >/etc/bash_completion.d/kubectl

echo "alias k=kubectl" >> ~/.bashrc
echo "alias kc=kubectl" >> ~/.bashrc

echo "source <(kubectl completion bash | sed s/kubeclt/k/g)" >> ~/.bashrc
echo "source <(kubectl completion bash | sed s/kubeclt/kc/g)" >> ~/.bashrc
echo 'source <(kubectl completion bash)' >>~/.bashrc


echo 'source <(kubectl completion bash)' >/etc/profile.d/k8s.sh
echo 'source <(kubectl completion bash | sed s/kubeclt/k/g)' >/etc/profile.d/k8s.sh
echo 'source <(kubectl completion bash | sed s/kubeclt/kc/g)' >/etc/profile.d/k8s.sh



source /etc/profile
source  ~/.bashrc



source <(kubectl completion bash)>
source <(kubectl completion bash | sed s/kubectl/k/g)
source <(kubectl completion bash | sed s/kubectl/kc/g)

```

```
如果直接在连接SSH窗口中输入source <(kubectl completion bash | sed s/kubectl/k/g)，则只对当前会话有效，断开连接后再次连接时需要重新执行。如果您希望每次连接时都自动执行该命令，可以将其添加到bashrc文件中。

打开MobaXterm连接的远程主机的bashrc文件。命令如下：

bash
Copy code
vim ~/.bashrc
在文件末尾添加以下行：

bash
Copy code
source <(kubectl completion bash | sed s/kubectl/k/g)
保存并关闭文件，然后退出远程主机。

再次连接到远程主机，该命令应该会自动执行。您可以通过输入echo $BASH_VERSION来验证是否成功执行。
```

# dashboard



```
wget  https://raw.githubusercontent.com/kubernetes/dashboard/v2.7.0/aio/deploy/recommended.yaml
vi recommended.yaml
	spec:
		type: NodePort
		ports:
			- port: 443
				targetPort: 8443
				nodePort: 31111
kubectl apply -f recommended.yaml

建一个账户
vi dashboard-adminuser.yaml
	apiVersion: v1
	kind: ServiceAccount
	metadata:
		name: admin-user
		namespace: kubernetes-dashboard
	---
	apiVersion: rbac.authorization.k8s.io/v1
	kind: ClusterRoleBinding
	metadata:
		name: admin-user
	roleRef:
		apiGroup: rbac.authorization.k8s.io
		kind: ClusterRole
		name: cluster-admin
	subjects:
	- kind: ServiceAccount
		name: admin-user
		namespace: kubernetes-dashboard
kubectl apply -f dashboard-adminuser.yaml

获取token
kubectl -n kubernetes-dashboard create token admin-user

访问https:master:31111

kubectl get pod -w 持续检测pod
#初始化容器 init-container 
	
# 容器传入命令+参数
 command: ["printenv"]
 args: ["HOSTNAME", "KUBERNETES_PORT"]
 使用环境变量来设置参数
	env:
	 - name: MESSAGE
		 value: "hello world"
	 - name: MESSAGE2
	 	 value: "hello world2"	 
	command: ["/bin/echo"]
	args: ["$(MESSAGE)"]
 $() 使用变量 
 $$() 不会解析变量
 未定义的变量变成字符串
```

# 权限用户
```
docker默认是以root映射用户进行启动,如果不使用--privileged将无法操作修改一些容器内的配置文件
kubectl参数:
securityContext:
  privileged: true
  runAsUser: 1000#运行的用户
  runAsGroup: 3000#运行的组
  fsGroup: 2000#操作系统用户
```

# 网络

```
#内部
1.通过环境变量注入,注意得先启动service,再启动pod.service的域名和IP才会注入到POD里面
2.通过域名DNS注册方式: hostname+subdomain注入
#外部
1. ClusterIP:
仅仅使用一个集群内部的IP地址 - 这是默认值。选择这个值意味着你只想这个服务在集群内部才可以
被访问到
2. NodePort:
在集群内部IP的基础上，在集群的每一个节点的端口上开放这个服务。你可以在任意:NodePort地址
上访问到这个服务。
3. LoadBalancer:
在使用一个集群内部IP地址和在NodePort上开放一个Service的基础上，还可以向云提供者申请一个
负载均衡器，将流量转发到已经以NodePort形式开发的Service上。
```

# 常用命令

```
kubectl get all  -A

kubectl api-resources

kubectl delete pods --all

kubectl get pod,svc,ep
```

# PV & PVC &Storage Class

![](/images/docker/464/3.png)

# 最佳配置实践

```
普通配置:
    定义配置时，请指定最新的稳定 API 版本。
    在推送到集群之前，配置文件应存储在版本控制中。 这允许您在必要时快速回滚配置更改。 它还有
    助于集群重新创建和恢复。
    使用 YAML 而不是 JSON 编写配置文件。虽然这些格式几乎可以在所有场景中互换使用，但 YAML
    往往更加用户友好。
    只要有意义，就将相关对象分组到一个文件中。 一个文件通常比几个文件更容易管理
Naked”Pods 与 ReplicaSet，Deployment 和 Jobs
    如果可能，不要使用独立的 Pods（即，未绑定到 ReplicaSet 或 Deployment 的 Pod）。 如果节点
    发生故障，将不会重新调度独立的 Pods。
    Deployment 会创建一个 ReplicaSet 以确保所需数量的 Pod 始终可用，并指定替换 Pod 的策略
    （例如 RollingUpdate)， 除了一些显式的restartPolicy: Never 场景之外，几乎总是优先考虑直接创
    建 Pod。 Job 也可能是合适的。    
服务
    在创建相应的后端工作负载（Deployment 或 ReplicaSet），以及在需要访问它的任何工作负载之前创
    建 服务。 当 Kubernetes 启动容器时，它提供指向启动容器时正在运行的所有服务的环境变量。 例如，
    如果存在名为 foo 的服务，则所有容器将在其初始环境中获得以下变量。
    这确实意味着在顺序上的要求 - 必须在 Pod 本身被创建之前创建 Pod 想要访问的任何 Service， 否则将
    环境变量不会生效。DNS 没有此限制。
    一个可选（尽管强烈推荐）的集群插件 是 DNS 服务器。DNS 服务器为新的 Services 监视
    Kubernetes API，并为每个创建一组 DNS 记录。 如果在整个集群中启用了 DNS，则所有 Pods 应该
    能够自动对 Services 进行名称解析。
    除非绝对必要，否则不要为 Pod 指定 hostPort。 将 Pod 绑定到hostPort时，它会限制 Pod 可以调
    度的位置数，因为每个 <hostIP, hostPort, protocol>组合必须是唯一的。 如果您没有明确指定
    hostIP 和 protocol，Kubernetes 将使用 0.0.0.0 作为默认 hostIP 和 TCP 作为默认 protocol。
    如果您只需要访问端口以进行调试，则可以使用 apiserver proxy或 kubectl port-forward。
    如果您明确需要在节点上公开 Pod 的端口，请在使用 hostPort 之前考虑使用 NodePort 服务。
    避免使用 hostNetwork，原因与 hostPort 相同。
    当您不需要 kube-proxy 负载均衡时，使用 无头服务
    (ClusterIP 被设置为 None)以便于服务发现。
使用标签
    定义并使用标签来识别应用程序 或 Deployment 的 语义属性，例如{ app: myapp, tier: frontend,
    phase: test, deployment: v3 }。 你可以使用这些标签为其他资源选择合适的 Pod； 例如，一个选择
    所有 tier: frontend Pod 的服务，或者 app: myapp 的所有 phase: test 组件。
    通过从选择器中省略特定发行版的标签，可以使服务跨越多个 Deployment。 Deployment 可以在
    不停机的情况下轻松更新正在运行的服务。
    Deployment 描述了对象的期望状态，并且如果对该规范的更改被成功应用， 则 Deployment 控制
    器以受控速率将实际状态改变为期望状态。
容器镜像
    imagePullPolicy和镜像标签会影响 kubelet 何时尝试拉取指定的镜像。
    imagePullPolicy: IfNotPresent：仅当镜像在本地不存在时才被拉取。
    imagePullPolicy: Always：每次启动 Pod 的时候都会拉取镜像。
    imagePullPolicy 省略时，镜像标签为 :latest 或不存在，使用 Always 值。
    imagePullPolicy 省略时，指定镜像标签并且不是 :latest，使用 IfNotPresent 值。
    imagePullPolicy: Never：假设镜像已经存在本地，不会尝试拉取镜像。
    说明： 在生产中部署容器时应避免使用 :latest 标记，因为这样更难跟踪正在运行的镜像版本，并
    且更难以正确回滚。
    说明： 底层镜像驱动程序的缓存语义能够使即便 imagePullPolicy: Always 的配置也很高效。 例
    如，对于 Docker，如果镜像已经存在，则拉取尝试很快，因为镜像层都被缓存并且不需要下载。    
使用 kubectl
    使用 kubectl apply -f 。 它在 中的所有 .yaml、.yml 和 .json 文件中查找 Kubernetes 配置，并将其
    传递给 apply。
    使用标签选择器进行 get 和 delete 操作，而不是特定的对象名称。
  
```

