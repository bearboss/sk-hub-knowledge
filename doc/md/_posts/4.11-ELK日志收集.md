title: ELK日志收集

date: 2021-05-25 15:20:37

tags: ELK日志收集

categories: ELK日志收集

copyright: true

sticky: 0

---

<span id="delete">

![](/images/banner/4.jpg)

</span>

<!--more-->
# 数据流向
```
## 网盘-软件工具-other-hexo-sit-ELK

springboot -> filebeat -> kafka -> logstash -> elasticsearch -> kibana

```

# 搭建KAFKA
```
version: '3'
services:
  zk:
    image: wurstmeister/zookeeper
    container_name: zk
    hostname: zk
    ports:
      - "2181:2181"
  kafka:
    image: wurstmeister/kafka
    container_name: kafka
    depends_on:
      - zk
    hostname: kafka
    ports:
      - 9092:9092
    environment:
      - KAFKA_BROKER_ID=0
      - KAFKA_ZOOKEEPER_CONNECT=zk:2181
      - ALLOW_PLAINTEXT_LISTENER=yes
      - KAFKA_ADVERTISED_LISTENERS=PLAINTEXT://192.168.137.104:9092 # 外部容器的地址
      - KAFKA_LISTENERS=PLAINTEXT://0.0.0.0:9092
  kafdrop:
    image: obsidiandynamics/kafdrop:latest
    container_name: kafdrop
    depends_on:
      - kafka
    ports:
      - 9000:9000
    environment:
      - KAFKA_BROKERCONNECT=kafka:9092
# 验证
#e:
#cd  JJDK/kafka/bin/windows
#kafka-console-producer.bat --broker-list bigdata04:9092 --topic campus-user-info-input-test
#kafka-console-consumer.bat --bootstrap-server bigdata04:9092 --topic campus-user-info-input-test --from-#beginning
```
# 搭建ES
```
mkdir -p /root/es_docker
cd  /root/es_docker
mkdir config logs plugins data

cd config

vi elasticsearch.yml

network.host: 0.0.0.0
network.bind_host: 0.0.0.0
network.publish_host: 192.168.137.104 # 这个地址特别注意,如果是127或者docker地址就无法连接
http.port: 9200
http.cors.enabled: true
http.cors.allow-origin: "*"

chmod -R +777 /root/es_docker

docker run --name elasticsearch -p 9200:9200  -p 9300:9300 \
       -e "discovery.type=single-node" \
       -e ES_JAVA_OPTS="-Xms128m -Xmx128m" \
       --privileged \
       -v /root/es_docker/config/elasticsearch.yml:/usr/share/elasticsearch/config/elasticsearch.yml\
       -v /root/es_docker/data:/usr/share/elasticsearch/data \
       -v /root/es_docker/plugins:/usr/share/elasticsearch/plugins \
       -d elasticsearch:6.6.0

docker run -d --name es-head -p 9100:9100 docker.io/mobz/elasticsearch-head:5
```
# 搭建kibana
```
mkdir -p /usr/local/kibana/config/kibana.yml
vi kibana.yml
  #设置Kibana映射端口
  server.port: 5601
  #设置网关地址
  server.host: "0.0.0.0"
  #设置Kibana实例对外展示的名称
  server.name: "kibana"
  #设置ES集群地址
  elasticsearch.hosts: ["http://192.168.137.104:9200"]
  #设置请求超时时长
  elasticsearch.requestTimeout: 120000
  #设置页面语言
  i18n.locale: "zh-CN"
docker run -d -p 5601:5601 -v /usr/local/kibana/config/kibana.yml:/usr/share/kibana/config/kibana.yml --restart=always --name kibana kibana:6.6.0
测试:http://bigdata04:5601
```
# spring boot项目配置log4f2
## 项目下载 [collector](..\resources\elk\collector.zip)
## log4j2.xml
```
<?xml version="1.0" encoding="UTF-8"?>
<Configuration status="INFO" schema="Log4J-V2.0.xsd" monitorInterval="600" >
    <Properties>
        <Property name="LOG_HOME">logs</Property>
        <property name="FILE_NAME">collector</property>
        <property name="patternLayout">[%d{yyyy-MM-dd'T'HH:mm:ss.SSSZZ}] [%level{length=5}] [%thread-%tid] [%logger] [%X{hostName}] [%X{ip}] [%X{applicationName}] [%F,%L,%C,%M] [%m] ## '%ex'%n</property>
    </Properties>
    <Appenders>
        <Console name="CONSOLE" target="SYSTEM_OUT">
            <PatternLayout pattern="${patternLayout}"/>
        </Console>  
        <RollingRandomAccessFile name="appAppender" fileName="${LOG_HOME}/app-${FILE_NAME}.log" filePattern="${LOG_HOME}/app-${FILE_NAME}-%d{yyyy-MM-dd}-%i.log" >
          <PatternLayout pattern="${patternLayout}" />
          <Policies>
              <TimeBasedTriggeringPolicy interval="1"/>
              <SizeBasedTriggeringPolicy size="500MB"/>
          </Policies>
          <DefaultRolloverStrategy max="20"/>         
        </RollingRandomAccessFile>
        <RollingRandomAccessFile name="errorAppender" fileName="${LOG_HOME}/error-${FILE_NAME}.log" filePattern="${LOG_HOME}/error-${FILE_NAME}-%d{yyyy-MM-dd}-%i.log" >
          <PatternLayout pattern="${patternLayout}" />
          <Filters>
              <ThresholdFilter level="warn" onMatch="ACCEPT" onMismatch="DENY"/>
          </Filters>              
          <Policies>
              <TimeBasedTriggeringPolicy interval="1"/>
              <SizeBasedTriggeringPolicy size="500MB"/>
          </Policies>
          <DefaultRolloverStrategy max="20"/>         
        </RollingRandomAccessFile>            
    </Appenders>
    <Loggers>
        <!-- 业务相关 异步logger -->
        <AsyncLogger name="com.bfxy.*" level="info" includeLocation="true">
          <AppenderRef ref="appAppender"/>
        </AsyncLogger>
        <AsyncLogger name="com.bfxy.*" level="info" includeLocation="true">
          <AppenderRef ref="errorAppender"/>
        </AsyncLogger>       
        <Root level="info">
            <Appender-Ref ref="CONSOLE"/>
            <Appender-Ref ref="appAppender"/>
            <AppenderRef ref="errorAppender"/>
        </Root>         
    </Loggers>
</Configuration>
```
# 配置filebeat按照规则处理日志
```
# 启动命令 ./filebeat -c filebeat.yml &
filebeat.prospectors:
- input_type: log
  paths:
    ## app-服务名称.log, 为什么写死，防止发生轮转抓取历史数据
    - /usr/local/logs/app-collector.log
  #定义写入 ES 时的 _type 值
  document_type: "app-log"
  multiline:
    #pattern: '^\s*(\d{4}|\d{2})\-(\d{2}|[a-zA-Z]{3})\-(\d{2}|\d{4})'   # 指定匹配的表达式（匹配以 2017-11-15 08:04:23:889 时间格式开头的字符串）
    pattern: '^\['                              # 指定匹配的表达式（匹配以 "{ 开头的字符串）
    negate: true                                # 是否匹配到
    match: after                                # 合并到上一行的末尾
    max_lines: 2000                             # 最大的行数
    timeout: 2s                                 # 如果在规定时间没有新的日志事件就不等待后面的日志
  fields:
    logbiz: collector
    logtopic: app-log-collector   ## 按服务划分用作kafka topic
    evn: dev
- input_type: log
  paths:
    - /usr/local/logs/error-collector.log
  document_type: "error-log"
  multiline:
    #pattern: '^\s*(\d{4}|\d{2})\-(\d{2}|[a-zA-Z]{3})\-(\d{2}|\d{4})'   # 指定匹配的表达式（匹配以 2017-11-15 08:04:23:889 时间格式开头的字符串）
    pattern: '^\['                              # 指定匹配的表达式（匹配以 "{ 开头的字符串）
    negate: true                                # 是否匹配到
    match: after                                # 合并到上一行的末尾
    max_lines: 2000                             # 最大的行数
    timeout: 2s                                 # 如果在规定时间没有新的日志事件就不等待后面的日志
  fields:
    logbiz: collector
    logtopic: error-log-collector   ## 按服务划分用作kafka topic
    evn: dev
output.kafka:
  enabled: true
  hosts: ["192.168.137.104:9092"]
  topic: '%{[fields.logtopic]}'
  partition.hash:
    reachable_only: true
  compression: gzip
  max_message_bytes: 1000000
  required_acks: 1
logging.to_files: true
```
# 配置logstash
## mkdir logstash-6.0.0/script
## vi logstash-script.conf
```
# 测试的启动命令 /usr/local/logstash-6.6.0/bin/logstash -f /usr/local/logstash-6.6.0/script/logstash-script.conf --verbose --debug
# 启动 nohup /usr/local/logstash-6.6.0/bin/logstash -f /usr/local/logstash-6.6.0/script/logstash-script.conf &   
# 查看日志 tail -200f nohup.out
## multiline 插件也可以用于其他类似的堆栈式信息，比如 linux 的内核日志。
input {
  kafka {
    topics_pattern => "app-log-.*"  ## kafka 主题 topic
    bootstrap_servers => "bigdata04:9092"  ## kafka 地址
    codec => json  ## 数据格式
    consumer_threads => 1  ## 增加consumer的并行消费线程数（数值可以设置为 kafka 的分片数）
    decorate_events => true
    group_id => "app-log-group" ## kafka 组别
  }
  kafka {
    topics_pattern => "error-log-.*"  ## kafka 主题 topic
    bootstrap_servers => "bigdata04:9092" ## kafka 地址
    codec => json  ## 数据格式
    consumer_threads => 1  ## 增加consumer的并行消费线程数（数值可以设置为 kafka 的分片数）
    decorate_events => true
    group_id => "error-log-group" ## kafka 组别
  }
}
 
filter {
  ## 时区转换，这里使用 ruby 语言，因为 logstash 本身是东八区的，这个时区比北京时间慢8小时，所以这里采用 ruby 语言设置为北京时区
  ruby {
    code => "event.set('index_time',event.timestamp.time.localtime.strftime('%Y.%m.%d'))"
  }

  ## [fields][logtopic] 这个是从 FileBeat 定义传入 Kafka 的
  if "app-log" in [fields][logtopic]{
    grok {
      ## 表达式,这里对应的是Springboot输出的日志格式
      match => ["message", "\[%{NOTSPACE:currentDateTime}\] \[%{NOTSPACE:level}\] \[%{NOTSPACE:thread-id}\] \[%{NOTSPACE:class}\] \[%{DATA:hostName}\] \[%{DATA:ip}\] \[%{DATA:applicationName}\] \[%{DATA:location}\] \[%{DATA:messageInfo}\] ## (\'\'|%{QUOTEDSTRING:throwable})"]
    }
  }
 
  ## [fields][logtopic] 这个是从 FileBeat 定义传入 Kafka 的
  if "error-log" in [fields][logtopic]{
    grok {
      ## 表达式
      match => ["message", "\[%{NOTSPACE:currentDateTime}\] \[%{NOTSPACE:level}\] \[%{NOTSPACE:thread-id}\] \[%{NOTSPACE:class}\] \[%{DATA:hostName}\] \[%{DATA:ip}\] \[%{DATA:applicationName}\] \[%{DATA:location}\] \[%{DATA:messageInfo}\] ## (\'\'|%{QUOTEDSTRING:throwable})"]
    }
  }
}
 
## 测试输出到控制台：
## 命令行输入 ./logstash -f /usr/local/logstash-6.4.3/script/logstash-script.conf  --verbose --debug
output {
  stdout { codec => rubydebug }
}
 
## elasticsearch：
output {
 
  if "app-log" in [fields][logtopic]{
    ## es插件
    elasticsearch {
      # es服务地址
      hosts => ["192.168.137.104:9200"]
      ## 索引名，%{index_time} 是由上面配置的 ruby 脚本定义的日期时间，即每天生成一个索引
      index => "app-log-%{[fields][logbiz]}-%{index_time}"
      # 是否嗅探集群ip：一般设置true
      # 只需要知道一台 elasticsearch 的地址，就可以访问这一台对应的整个 elasticsearch 集群
      sniffing => true
      # logstash默认自带一个mapping模板，进行模板覆盖
      template_overwrite => true
    }
  }
  
  if "error-log" in [fields][logtopic]{
    elasticsearch {
      hosts => ["192.168.137.104:9200"]
      index => "error-log-%{[fields][logbiz]}-%{index_time}"
      sniffing => true
      template_overwrite => true
    } 
  }
}
```
# 测试
```
springboot访问
http://bigdata03:8001/index - 正常日志
http://bigdata03:8001/err - 错误日志
- ES存储日志,kibana可查看收集的日志

kibana 配置 
management -> 创建索引规则 (Index Patterns) -> Create index pattern 
-> Step 1 of 2: Define index pattern  "app-log*" -> 
-> Time Filter field name "currentDateTime" 
再配置一个error-log*

回到主页discover,可看到对应的规则数据

```
# watcher 告警
## kibana License Management 申请一个许可证30天即可出现watcher管理 
```
## 创建一个watcher,比如定义一个trigger 每个10s钟看一下input里的数据
## 创建一个watcher,比如定义一个trigger 每个5s钟看一下input里的数据
PUT _xpack/watcher/watch/error_log_collector_watcher
{
  "trigger": {
    "schedule": {
      "interval": "5s"
    }
  },
  "input": {
    "search": {
      "request": {
        "indices": ["<error_log_collector-{now+8h/d}>"], # 这里的时间表达式有问题,如果换成是直接的数据库表名error-log-collector-2023.10.23就没有问题
        "body": {
          "size": 10,
          "query": {
            "bool": {
              "must": [
                  {
                    "term": {"level.keyword": "ERROR"}
                  }
              ],
              "filter": {
                "range": {
                    "currentDateTime": {
                    "gt": "now-30s" , "lt": "now"
                  }
                }
              }
            }
          }
        }
      }
    }
  },

  "condition": {
    "compare": {
      "ctx.payload.hits.total": {
        "gt": 0
      }
    }
  },

  "transform": {
    "search": {
      "request": {
        "indices": ["<error-log-collector-{now+8h/d}>"],
        "body": {
          "size": 10,
          "query": {
            "bool": {
              "must": [
                  {
                    "term": {"level.keyword": "ERROR"}
                  }
              ],
              "filter": {
                "range": {
                    "currentDateTime": {
                    "gt": "now-30s" , "lt": "now"
                  }
                }
              }
            }
          },
          "sort": [
            {
                "currentDateTime": {
                    "order": "desc"
                }
            }
          ]
        }
      }
    }
  },
  "actions": {
    "test_error": {
      "webhook" : {
        "method" : "POST",
        "url" : "http://192.168.137.103:8001/accurateWatch",
        "body" : "{\"title\": \"异常错误告警\", \"applicationName\": \"{{#ctx.payload.hits.hits}}{{_source.applicationName}}{{/ctx.payload.hits.hits}}\", \"level\":\"告警级别P1\", \"body\": \"{{#ctx.payload.hits.hits}}{{_source.messageInfo}}{{/ctx.payload.hits.hits}}\", \"executionTime\": \"{{#ctx.payload.hits.hits}}{{_source.currentDateTime}}{{/ctx.payload.hits.hits}}\"}"
      }
    }
 }
}

# 查看一个watcher
# 
GET _xpack/watcher/watch/error_log_collector_watcher


#删除一个watcher
DELETE _xpack/watcher/watch/error_log_collector_watcher

#执行watcher
# POST _xpack/watcher/watch/error_log_collector_watcher/_execute

#查看执行结果
GET /.watcher-history*/_search?pretty
{
  "sort" : [
    { "result.execution_time" : "desc" }
  ],
  "query": {
    "match": {
      "watch_id": "error_log_collector_watcher"
    }
  }
}

GET error-log-collector-2019.09.18/_search?size=10
{

  "query": {
    "match": {
      "level": "ERROR"
    }
  }
  ,
  "sort": [
    {
        "currentDateTime": {
            "order": "desc"
        }
    }
  ] 
}


GET error-log-collector-2019.09.18/_search?size=10
{

  "query": {
    "match": {
      "level": "ERROR"
    }
  }
  ,
  "sort": [
    {
        "currentDateTime": {
            "order": "desc"
        }
    }
  ] 
}
```
