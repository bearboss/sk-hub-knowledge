title: Docker-219

date: 2021-05-25 15:20:37

tags: Docker

categories: Docker

copyright: true

sticky: 0

---

<span id="delete">

![](/images/banner/4.jpg)

</span>

<!--more-->

# docker 搭建数据库集群
![](/images/docker/haproxy.png)
```
Replication方案    
    异步处理数据一致性
    读写分离，单向同步，主写到从节点
PXC方案（Percona XtraDB Cluster）
    强一致性，同步同步数据
    每个节点都可以读写
#下载
docker pull docker.io/percona/percona-xtradb-cluster:5.7.21
#改名
docker tag docker.io/percona/percona-xtradb-cluster pxc
#删除镜像
docker rmi docker.io/percona/percona-xtradb-cluster:5.7.21
#创建五个卷
docker volume create --name v1
docker volume create --name v2
docker volume create --name v3
docker volume create --name v4
docker volume create --name v5
#创建网络
docker network create --subnet 172.18.0.0/24 net1
#启动
docker run -di -p 3406:3306 -v v1:/var/lib/mysql -e MYSQL_ROOT_PASSWORD=123456 -e CLUSTER_NAME=PXC -e XTRABACKUP_PASSWORD=123456 --privileged=true --name=node1 --net=net1 --ip 172.18.0.2 pxc
docker run -di -p 3407:3306 -v v2:/var/lib/mysql -e MYSQL_ROOT_PASSWORD=123456 -e CLUSTER_NAME=PXC -e XTRABACKUP_PASSWORD=123456 -e CLUSTER_JOIN=node1 --privileged=true --name=node2 --net=net1 --ip 172.18.0.3 pxc
docker run -di -p 3408:3306 -v v3:/var/lib/mysql -e MYSQL_ROOT_PASSWORD=123456 -e CLUSTER_NAME=PXC -e XTRABACKUP_PASSWORD=123456 -e CLUSTER_JOIN=node1 --privileged=true --name=node3 --net=net1 --ip 172.18.0.4 pxc
docker run -di -p 3409:3306 -v v4:/var/lib/mysql -e MYSQL_ROOT_PASSWORD=123456 -e CLUSTER_NAME=PXC -e XTRABACKUP_PASSWORD=123456 -e CLUSTER_JOIN=node1 --privileged=true --name=node4 --net=net1 --ip 172.18.0.5 pxc
docker run -di -p 3410:3306 -v v5:/var/lib/mysql -e MYSQL_ROOT_PASSWORD=123456 -e CLUSTER_NAME=PXC -e XTRABACKUP_PASSWORD=123456 -e CLUSTER_JOIN=node1 --privileged=true --name=node5 --net=net1 --ip 172.18.0.6 pxc

#haproxy tcp代理
docker pull haproxy:1.8.8
#改名
docker tag docker.io/haproxy:1.8.8 haproxy
#删除
docker rmi docker.io/haproxy:1.8.8
#创建haproxy.cfg文件
mkdir -p /home/soft/haproxy
touch haproxy.cfg
    global
    #工作目录
    chroot /usr/local/etc/haproxy
    #日志文件，使用rsyslog服务中local5日志设备（/var/log/local5），等级info
    log 127.0.0.1 local5 info
    #守护进程运行
    daemon

    defaults
        log	global
        mode	http
        #日志格式
        option	httplog
        #日志中不记录负载均衡的心跳检测记录
        option	dontlognull
        #连接超时（毫秒）
        timeout connect 5000
        #客户端超时（毫秒）
        timeout client  50000
        #服务器超时（毫秒）
        timeout server  50000
    
    #监控界面	
    listen  admin_stats
        #监控界面的访问的IP和端口
        bind  0.0.0.0:8888
        #访问协议
        mode        http
        #URI相对地址
        stats uri   /dbs
        #统计报告格式
        stats realm     Global\ statistics
        #登陆帐户信息
        stats auth  admin:abc123456
    #数据库负载均衡
    listen  proxy-mysql
        #访问的IP和端口
        bind  0.0.0.0:3306  
        #网络协议
        mode  tcp
        #负载均衡算法（轮询算法）
        #轮询算法：roundrobin
        #权重算法：static-rr
        #最少连接算法：leastconn
        #请求源IP算法：source 
        balance  roundrobin
        #日志格式
        option  tcplog
        #在MySQL中创建一个没有权限的haproxy用户，密码为空。Haproxy使用这个账户对MySQL数据库心跳检测
        option  mysql-check user haproxy
        server  MySQL_1 172.18.0.2:3306 check weight 1 maxconn 2000  
        server  MySQL_2 172.18.0.3:3306 check weight 1 maxconn 2000  
        server  MySQL_3 172.18.0.4:3306 check weight 1 maxconn 2000 
        server  MySQL_4 172.18.0.5:3306 check weight 1 maxconn 2000
        server  MySQL_5 172.18.0.6:3306 check weight 1 maxconn 2000
        #使用keepalive检测死链
        option  tcpka  
#启动docker
    docker run -it -d -p 4001:8888 -p 4002:3306 -v /home/soft/haproxy:/usr/local/etc/haproxy --name h1 --privileged --net=net1 --ip 172.18.0.7 haproxy 
#进入容器
    docker exec -it h1 bash
#启动haproxy
    haproxy -f /usr/local/etc/haproxy/haproxy.cfg
#在数据执行
    create user 'haproxy'@'%' IDENTIFIED BY '';
#在浏览器执行
    http://bigdata02:4001/dbs admin/abc123456
#效果1
    #停掉node1，再重启 ，注意 pxc节点单节点重启无效，必须要删除所有节点再重启
        docker ps -a
        docker stop node1
    #保证强一致性
        docker stop node1 node2 node3 node4 node5
        docker rm node1 node2 node3 node4 node5
        rm -rf /var/lib/docker/volumes/v1/_data/grastate.dat
        rm -rf /var/lib/docker/volumes/v2/_data/grastate.dat
        rm -rf /var/lib/docker/volumes/v3/_data/grastate.dat
        rm -rf /var/lib/docker/volumes/v4/_data/grastate.dat
        rm -rf /var/lib/docker/volumes/v5/_data/grastate.dat
    #再执行容器初始化即可
    注意：PXC集群只有在超过一半数量的节点宕机，集群才会不可用，这是为了避免异地机房部署PXC集群，因为网络故障，导致一个PXC集群分裂成两个集群。所以说挂掉一两个节点没什么问题，只要不超过一半的节点就行。
#效果2
    #停掉node1，再重启 ，需要让node1跟随其他节点重启加上 -e CLUSTER_JOIN=node2参数重启

# haproxy keepalived双机热备
    #启动h2
    docker run -it -d -p 4003:8888 -p 4004:3306 -v /home/soft/haproxy:/usr/local/etc/haproxy --name h2 --privileged --net=net1 --ip 172.18.0.8 haproxy
    # 分配配置h1 h2的  keepalived
    docker exec -it h1 bash
    apt-get update
    apt-get -y install keepalived vim iputils-ping
    vim /etc/keepalived/keepalived.conf
        vrrp_instance VI_1 {
            state MASTER
            interface eth0
            virtual_router_id 51
            priority 100
            advert_int 1
            authentication {
                auth_type PASS
                auth_pass 123456
            }
            virtual_ipaddress {
                172.18.0.201
            }
        }
    service keepalived start 
    #exit检验
    ping 172.18.0.201
    telnet 172.18.0.201 8888
    telnet 172.18.0.201 3306
   #重复配置H2的keepalived
   。。。同上 
   #真实机器安装keepalived
   yum ‐y install keepalived
   vi /etc/keepalived/keepalived.conf
       vrrp_instance VI_1 {
            state MASTER
            interface eth0 #网卡的名字
            virtual_router_id 51
            priority 100
            advert_int 1
            authentication {
                auth_type PASS
                auth_pass 123456
            }
            virtual_ipaddress {
                172.21.27.157（真实机器的同网段IP）
            }
        }
        virtual_server 172.21.27.157 8888 {
            delay_loop 3
            lb_algo rr
            lb_kind NAT
            persistence_timeout 50
            protocol TCP
            real_server 172.18.0.201 8888 {
                weight 1
            }
        }
        virtual_server 172.21.27.157 3306 {
            delay_loop 3
            lb_algo rr
            lb_kind NAT
            persistence_timeout 50
            protocol TCP
            real_server 172.18.0.201 3306 {
                weight 1
            }
        }
   #启动     
   /bin/systemctl restart keepalived.service    
   #检验
   ping  172.21.27.157  
   telnet  172.21.27.157 3306  
```
# docker数据库备份
![](/images/docker/冷备份.png)
![](/images/docker/热备份.png)
```
#冷备份是停机备份
#热备份LVM需要锁表
#XtraBackup 
    支持全量+增量
    不锁表，不会打断正在执行的事务
    备份具有压缩属性


#1.数据库备份
docker volume create backup
docker stop node1
#增加备份目录、
docker run -di -p 3406:3306 -v v1:/var/lib/mysql -v backup:/data -e MYSQL_ROOT_PASSWORD=123456 -e CLUSTER_NAME=PXC -e XTRABACKUP_PASSWORD=123456 -e CLUSTER_JOIN=node2 --privileged=true --name=node1 --net=net1 --ip 172.18.0.2 pxc
#安装XtraBackup工具
docker exec -it node1 bash
apt-get update
apt-get -y install percona-xtrabackup-24
#全量备份命令
innobackupex --user=root --password=123456 /data/backup/full
#查看备份文件
docker inspect backup
cd /var/lib/docker/volumes/backup/_data

#2.数据库还原只有冷还原
docker stop node2
docker stop node3
docker stop node4
docker stop node5
docker rm node2
docker rm node3
docker rm node4
docker rm node5
#删除数据
rm ‐rf /var/lib/mysql/*
#清空事务
innobackupex --user=root --password=123456 --apply‐back /data/backup/full/2023-03-09_03-04-23/
#还原数据
innobackupex --user=root --password=123456 --copy-back /data/backup/full/2023-03-09_03-04-23/
#最后重新创建其余4个节点，组件PXC集群
```
# docker redis集群
![](/images/docker/redis.png)
```
    # master写数据，计算是不是在本master，是就slave写数据 ，不是就发送对应的master
    docker pull yyyyttttwwww/redis
    docker network create ‐‐subnet=172.19.0.0/16 net2
    docker run -it -d --name r1 -p 5001:6379 --net=net2 --ip 172.19.0.2 redis bash
    docker run -it -d --name r2 -p 5002:6379 --net=net2 --ip 172.19.0.3 redis bash
    docker run -it -d --name r3 -p 5003:6379 --net=net2 --ip 172.19.0.4 redis bash
    docker run -it -d --name r4 -p 5004:6379 --net=net2 --ip 172.19.0.5 redis bash
    docker run -it -d --name r5 -p 5005:6379 --net=net2 --ip 172.19.0.6 redis bash
    docker run -it -d --name r6 -p 5006:6379 --net=net2 --ip 172.19.0.7 redis bash
    #进入r1节点
    docker exec -it r1 bash
    vi /usr/redis/redis.conf
        #以后台进程运行
        daemonize yes
        #开启集群
        cluster-enabled yes
        #集群配置
        cluster-config-file nodes-6379.conf
        #超时时间
        cluster-node-timeout 15000
        #开启AOF
        appendonly yes
        
    cd /usr/redis/src
    ./redis-server ../redis.conf
    
    #在r1节点上执行下面的指令
    cd /usr/redis/src
    mkdir -p ../cluster
    cp redis-trib.rb ../cluster/
    cd ../cluster
    #创建Cluster集群
    ./redis‐trib.rb create --replicas 1 172.19.0.2:6379 172.19.0.3:6379 172.19.0.4:6379 172.19.0.5:6379 172.19.0.6:6379 172.19.0.7:6379
```
# docker真实机器获取容器启动命令
```
    1. docker inspect 容器ID
    2. docker ps -a --no-trunc 容器ID
    3. get_command_4_run_container
        docker pull cucker/get_command_4_run_container
        docker tag docker.io/cucker/get_command_4_run_container command
        docker rmi docker.io/cucker/get_command_4_run_container:latest
        #快捷命令
        echo "alias docker-info='docker run --rm -v /var/run/docker.sock:/var/run/docker.sock cucker/command'" >> ~/.bashrc \
        && \
        . ~/.bashrc
        #刷新
        source ~/.bashrc
        #检验
        docker-info nginx
```
# docker部署后端服务
```
    #java服务启动
    docker volume create --name j1
    docker run -it -d --name j1 -v j1:/home/soft --net=host java
    docker exec -it j1 bash
    nohup java -jar /home/soft/renren_fast.jar &
    
    #反向代理
    docker pull nginx 
    vi /home/n1/nginx.conf
        worker_processes 2;
        error_log /var/log/nginx/error.log;
        pid /var/run/nginx.pid;
        events {
            worker_connections 1024;
        }
        http {
            include /etc/nginx/mime.types;
            default_type application/octet‐stream;
            log_format main '$remote_addr ‐ $remote_user [$time_local] "$request" '
            '$status $body_bytes_sent "$http_referer" '
            '"$http_user_agent" "$http_x_forwarded_for"';
            access_log /var/log/nginx/access.log;
            sendfile on;
            #tcp_nopush on;
            keepalive_timeout 65;
            upstream tomcat {
                server 172.21.27.155:6001;
                server 172.21.27.155:6002;
            }
            server {
                listen 6101;
                server_name 172.21.27.155;
                error_log /var/log/nginx/error.log;
                location / {
                        proxy_pass http://tomcat/;
                        index index.html index.htm;
                }
            }
        }
    #启动    
    docker run -dti \
     --name n1 \
     --privileged  \
     --network=host \
     -v /home/n1/nginx.conf:/etc/nginx/nginx.conf \
     nginx
    #检验
    http://172.21.27.155:6101/
```
# docker swarm
```

提供集群数量保证+共享网络互通能力
集群数量保: 宕机一个节点，自动拉起一个节点（不适合数据库有状态的节点使用，新启动数据同步会造成压力）

# 管理者地址  --listen-addr
# 广播地址   --advertise-addr
docker swarm init --listen-addr 172.21.27.155:7777 --advertise-addr 172.21.27.155:7778

docker swarm join-token manager

docker swarm join-token worker

#docker node ls 节点信息
#docker network ls 共享网络
docker network create -d overlay --attachable swarm_test
# swarm 共享网络
docker run -idt --net=swarm_test

# 退出网络
主动退出 docker swarm leave --force
被动退出：manager节点必须降级成为worker节点，然后再删除
docker node demote xxxx
docker stop xx
docker node rm xx
 
```

# docker portainer图形化界面
```
docker pull  portainer/portainer
vi /etc/sysconfig/docker
OPTIONS='-Htcp://0.0.0.0:2375 -H unix:///var/run/docker.sock'
/bin/systemctl restart docker.service
docker run -itd -p9000:9000 portainer/portainer -H tcp://172.21.27.155:2375
```
