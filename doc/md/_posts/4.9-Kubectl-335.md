---
typora-root-url: ..
---

title: Docker-335

date: 2021-05-25 15:20:37

tags: Docker

categories: Docker

copyright: true

sticky: 0

---

<span id="delete">

![](/images/banner/4.jpg)

</span>

<!--more-->
# 安装端口工具
```
yum -y install net-tools

netstat -ntlp|grep 80

netstat -nao | findstr
```
# kubeasz安装k8s
https://github.com/easzlab/kubeasz/blob/master/docs/setup/quickStart.md

# kubectl
![](/images/docker/k8s1.png)
![](/images/docker/k8s2.png)
![](/images/docker/k8s3.png)
![](/images/docker/k8s4.png)
![](/images/docker/k8s5.png)
![](/images/docker/k8s6.png)
![](/images/docker/k8s7.png)
![](/images/docker/k8s8.png)
# yml定义
```
apiVersion: apps/v1  # api版本
kind: xxxx   # 要创建的资源类型，如Deployment/Pod/ReplicaSet/StatefulSet/DaemonSet/Job/Cronjob/Service/Ingress...
metadata:    # 元数据对象，该资源的基本属性和信息
  name: xxx  # 定义该资源的名称
  namespace: xxx  # 命名空间，默认放到default空间
  lables:    # 标签，在下一行定义键值对，可以是多对键值对
    xxx: xxxx
    xxx: xxxx
  annotations # 资源注解
    xxx: xxxx
spec:   # 定义期望状态，详细的创建信息 # 描述该资源的创建信息，对应kind资源类型的信息
  containers:  # 容器列表
  - name: xxx   # 容器名
    image: xxxx  # 容器镜像
  revisionHistoryLimit: 10  # 回滚时会用到，用来保留最近10的版本
  replicas: 2  # 创建2个应用实例
  selector:  
  # 标签选择器，与上面的标签共用，这个部分是17版本开始加的，必须与上面的labels对应
    matchLabels: # 选择包含标签app:nginx的资源
    # 正确的Deployment,让matchLabels 和template.metadata.lables完全匹配才能不报错
    # 直接不写spec.mathlabels创建直接报错缺少缺少必要字段selector
    # 当把matchLables匹配的和下面pod模板不相对应,也会直接报错：选择的标签和模板标签不匹配
    # matchLabel是pod的标签选择器。 由此选择其pod的现有ReplicaSet(副本集)将受此部署影响的副本。
      app: nginx

# 种类介绍
kind: Endpoints  一个外部的 mysql,可以把外部的链接到k8s系统中

kind: Service  Service 部署一个内部虚拟IP，其他 deployment 可以链接。

kind: Deployment  
kind: Pod  
如果单单创建一个pod类型是不够的，应为该pod应用挂了，就不会重启了。
如果创建一个Deployment类型，可以创建管理该pod应用，挂了都会给你重启

#找到对应的版本apiVersion

kubectl explain Pod

```

#安装ingress-nginx
```
wget  https://raw.githubusercontent.com/kubernetes/ingress-nginx/controller-v1.7.0/deploy/static/provider/cloud/deploy.yaml
修改镜像的路径
registry.cn-hangzhou.aliyuncs.com/google_containers/nginx-ingress-controller:v1.7.0
registry.cn-hangzhou.aliyuncs.com/google_containers/kube-webhook-certgen:v20230312-helm-chart-4.5.2-28-g66a760794
kubectl delete -f deploy.yaml
kubectl apply -f deploy.yaml
# 有个type: LoadBalancer 改成 type: NodePort
在 ingress 控制器的Deployment sec:templete:sec下 定义中加入：hostNetwork: true
kubectl get pods -A

#ingress配置

apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx1
spec:
  selector:
    matchLabels:
      app: nginx1
  replicas: 1
  template:
    metadata:
      labels:
        app: nginx1
    spec:
      containers:
      - name: nginx1
        image: nginx:alpine
        ports:
        - containerPort: 80
        resources:
          requests:
            memory: "180Mi"
            cpu: "500m"
        limits:
            memory: "10Mi"
            cpu: "100m"
        
---
apiVersion: v1
kind: Service
metadata:
  name: nginx
spec:
  selector:
    app: nginx
  ports:
  - protocol: TCP
    port: 80
    targetPort: 80
---
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: nginx
  namespace: default
  annotations:
    kubernetes.io/ingress.class: "nginx"
    nginx.ingress.kubernetes.io/use-regex: "true"
spec:
  ingressClassName: nginx
  rules:
  - host: nginx.test.com
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: nginx
            port:
              number: 80
#配置完成查看端口
kubectl get svc -n ingress-nginx
kc get ing
	demo-localhost   nginx   nginx.test.com   10.1.49.201   80      4m36s

通过10.1.49.201+端口去访问
#注意nginx配置只有80端口,改成其他端口将无法访问,除非自己再去加一个nginx.conf配置

POD_NAMESPACE=ingress-nginx
POD_NAME=$(kubectl get pods -n $POD_NAMESPACE -l app.kubernetes.io/name=ingress-nginx --field-selector=status.phase=Running -o name)
kubectl exec $POD_NAME -n $POD_NAMESPACE -- /nginx-ingress-controller --version
```

# kind: CronJob 定时任务
```
apiVersion: batch/v1beta1
kind: CronJob
metadata:
    name; cronjob-demo
spec:
    schedule: "*/1 * * * *"
    successfulJobsHistoryLimit: 3
    suspend: false
    concurrencyPolicy: Forbid
    failedJobsHistoryLimit: 1
    jobTemplate:
      spec:
        template:
          metadata:
            labels:
              app: conjob-demo
            spec:
              restartPolicy: Never
              containers:
                name: cronjob-demo
                image: hub.mooc.com/kubernetes/cronjob:v1
```

# 基本概念
```
#命名空间 -n
    不同命名空间的限制的是名字,不限制IP
    kubectl get namespaces
    #创建命名空间
        kubectl create ns dev
        #配置文件
            apiVersion: v1
            kind: Namespace
            metadata:
              name: dev
    #权限 - 只看某一个命名空间
        --namespace=dev set-context=dev
        
#Resources
    requests: 请求希望的资源大小  cpu: 10m + memeory:10Gi /10Mi
    limits: 限制最大最小 
    kind: Limitrange 配置文件 总体限制
    kind: ResourceQuota 资源配额
    
#Label
    kubectl get pods -l app-demo -n dev
    kubectl get pods -l 'group in (dev,test)' -n dev
    
    nodeSelecter: - node选择
        type: ssd 
    kubectl label node xxx type=ssd
    
#livenessProbe 健康检查
    # 脚本检查
    livenessProbe:
      exec:
        command:
        - /bin/sh
        - -c
        - ps -ef|grep java|grep -v grep
      initialDelaySeconds: 10
      periodSeconds: 10
      failureThreshold: 2
      successThreshold: 1
      timeoutSeconds: 5
     # http检查
       httpGet:
        path: /ex/index.html
        port: 8080
        scheme: HTTP
     # TCP检查  
       tcpSocket:
        port: 8080
     # 解决程序还没有启动就被加入轮询端口组
     readinessProbe:
       httpGet:
        path: /ex/index.html
        port: 8080
        scheme: HTTP
        
#affinity亲和性 ,用于节点部署在那些机器上
    nodeAffinify节点亲和性
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
            matchExpressions:
            - key: beta.kubernetes.io/arch
              operator: In
              values:
              - amd64
        preferredDuringSchedulingIgnoredDuringExecution:
          weight: 1
          preference:
            matchExpressions
            - key: disktype
              operator: NotIn
              values:
              - ssd  
     affinity: #pod亲和性
        podAffinity:
#污点 
    kubectl taint xxx  gpu=true:NoSchedule
    tolerations:
      key: "gpu"      
      operator: "Equal"
      value: "true"
      effect: "NoSchedule"  
      
#部署的策略
    while sleep 0.2;do curl "http://baidu.com";echo "";done
    # rolling update 滚动更新
        strategy:
          type: RollingUpdate
          rollingUpdate:
            maxSurge: 25%
            maxUnavailable: 25%
        kubectl rollout pause deploy xxx -n dev 暂停部署
        kubectl rollout resume deploy xxx -n dev 恢复部署
        kubectl rollout undo deploy xx -n dev 回滚上一个版本
            
    # recreate 暂停重启
        strategy:
          type: Recreate
          
    #蓝绿部署: label selecter
        通过修改service的selecter标签,并应用,将请求切换
    #金丝雀: 
        可以同时访问旧版本和新版本.AB测试,将蓝绿部署的版本标签去掉
        
#pod
    #修改pod的host
        hostAliases:
         ip: "xxx"
         hostnames:
         - xx.com
           ccc.com
    hostNetwork: true
    hostPID: true
    #容器的生命周期
    lifecycle:
      postStart:
        exec:
          commend: ["/bin/sh","-c","echo weeccc >> /var/lib/log"]
      preStop:
        exec:
          commend: ["/bin/sh","-c","echo weeccc >> /var/lib/log && sleep 3"]
    #pod的周期
    Pendding -> containerCreating -> Running -> Succeeded/Failed  
    Ready CrashLoopBackOff Ukonwn
    #ProhectedVolums
        secrets ConfigMap downwardAPI
        
#Ingress控制流量
     seesion保持
     cooike header 权重 
     
#共享存储 PV PVC 

     kind: PersistentVolumeClaim
 
     kind: PersistentVolume
     spec:
       capacity:
         storage: 10Gi
       accessModes:
         ReadWriteOnce
       nfs:
         path: "/tmp"
         server: 127.0.0.1   
          
# StatefulSet
  pod的顺序性 + 区分持久存储        
  
# DaemonSet 
  DaemonSet在每一个node节点上只调度一个Pod，因此无需指定replicas的个数
  
#Deployment
    apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: nginx-deployment
      labels:
        app: nginx
    spec:
      replicas: 3
      selector:
        matchLabels:
          app: nginx
      template:
        metadata:
          labels:
            app: nginx
        spec:
          containers:
          - name: nginx
            image: nginx:1.12.2
            ports:
            - containerPort: 80
    kubectl get deployment -o wide  
    //通过inamge更新
    kubectl set image deployment nginx-deployment nginx=nginx:1.13
    //直接编辑文件进行更新
    kubectl edit deployment nginx-deployment
    
    kubectl rollout history  deployment nginx-deployment
    kubectl rollout undo  deployment nginx-deployment
    kubectl expose deployment nginx-deployment type=NodePort

#Services
    kubectl expose命令
    通过YML创建
    #端口
    kubectl port-forward nginx 8080:80
    三种类型:
        ClusterIP  cluster能访问
        NodePort 映射到任意一个Node
        LoadBalaner 外部厂商
    kubectl expose pods nginx-pod

    kubectl get svc
    kubectl config get-contexts
    kubectl config use-context minikube
    # shell补全
    kubctl completion zsh
    source <(kubctl completion zsh)
    labels 是指resources的标签,可以多个
# kubectl create secret 
    kubectl create secret generic mysql-pass  --from-literal=pawword=impss
    kubectl get secret
```
# helm
```
### 1. helm安装
wget https://get.helm.sh/helm-v3.11.3-linux-amd64.tar.gz
tar -zxvf helm-v2.13.1-linux-amd64.tar.gz
mv linux-amd64/helm /usr/local/bin/
# 没配置环境变量的需要先配置好
export PATH=$PATH:/usr/local/bin/
# 验证
helm version

### 2. Tiller安装

Tiller 是以 Deployment 方式部署在 Kubernetes 集群中的，由于 Helm 默认会去 storage.googleapis.com 拉取镜像
# 指向阿里云的仓库
$ helm init --client-only --stable-repo-url https://aliacs-app-catalog.oss-cn-hangzhou.aliyuncs.com/charts/
$ helm repo add incubator https://aliacs-app-catalog.oss-cn-hangzhou.aliyuncs.com/charts-incubator/
$ helm repo update
# 因为官方的镜像无法拉取，使用-i指定自己的镜像
$ helm init --service-account tiller --upgrade -i registry.cn-hangzhou.aliyuncs.com/google_containers/tiller:v2.13.1  --stable-repo-url https://kubernetes.oss-cn-hangzhou.aliyuncs.com/charts
# 创建TLS认证服务端
$ helm init --service-account tiller --upgrade -i registry.cn-hangzhou.aliyuncs.com/google_containers/tiller:v2.13.1 --tiller-tls-cert /etc/kubernetes/ssl/tiller001.pem --tiller-tls-key /etc/kubernetes/ssl/tiller001-key.pem --tls-ca-cert /etc/kubernetes/ssl/ca.pem --tiller-namespace kube-system --stable-repo-url https://kubernetes.oss-cn-hangzhou.aliyuncs.com/charts

### 3. 给Tiller授权
因为 Helm 的服务端 Tiller 是一个部署在 Kubernetes 中的 Deployment，它会去访问ApiServer去对集群进行操作。目前的 Tiller 部署时默认没有定义授权的 ServiceAccount，这会导致访问 API Server 时被拒绝。所以我们需要明确为 Tiller 部署添加授权。
# 创建serviceaccount
$ kubectl create serviceaccount --namespace kube-system tiller
# 创建角色绑定
$ kubectl create clusterrolebinding tiller-cluster-rule --clusterrole=cluster-admin --serviceaccount=kube-system:tiller

### 4. 验证
```bash
# 查看Tiller的serviceaccount，需要跟我们创建的名字一致：tiller
$ kubectl get deploy --namespace kube-system tiller-deploy -o yaml|grep serviceAccount

# 验证pods
$ kubectl -n kube-system get pods|grep tiller

# 验证版本
$ helm version

```